--------------------------DATABASES--------------------------------------------
DB provides Organised persistent storage

Questions to choose right database based on your architecture
read heavy , write heavy , balanced workload , how much Throughput need
scaling or fluctuate
how much data to sore and how long, will it grow, avg object size
data durability
latency requirements 
concurrent users
data model, joins , structured, semi structure
strong schema,,reporting
License costs , migration to cloud

Availability 
app will not communicate with DB
measured as % of time app provides its operations as expected
4 9's is very good

Durability
data will be lost
measured as will my data be available after 10 years
11 9's is very good

Add DB snapshot after an hour to other data centre
Add transaction log (changes after taking snapshot)
You can setup DB from latest snapshot and apply transaction logs

Use Standby DB in 2nd DC with Sync Replication
**Create snapshot from standby DB so that performance will not impact

DownTime 
99.95%
22 mins in a month

99.99% 4 9's
4 and .5  mins in a month

99.999% 5 9's
26 sec in a month

11 9's durability means
store 1 million files for 10 million years, you would expect to lose 1 file

Increase Availability
standbys in Multiple AZ and multiple Regions

Increase Durability 
Multiple copies of data (standby,transaction logs, replicas) in multiple Az and Regions

RTO (Recovery Time Objective)
is the maximum length of time after an outage that your company is willing to wait for the recovery process to finish downtime

RPO Recovery point objective
is the maximum amount of data loss your company is willing to accept as measured in time

very small data loss (RPO 1 min)
very small data loss (RTO 5 min)
Hot Standby (automatically sync data, failover),standby ready

very small data loss (RPO 1 min)
downtime can be tolerate (RTO 15 min)
Warm Standby (automatically sync data,standby with min infra)

Data is critical (RPO 1 min)
downtime (can be few hours)
Regular data snapshots and transaction logs
crate DB from snapshots/transaction logs when a failure happs

Data Can be loss (cached data)
failover to a completely server

Scenario 

Reporting/Analytic is required on DB 
issue: will impact DB performance
Sol:
vertical scale 
DB cluster (expensive)
Create read replicas (async replication)

Actual App will read/write to DB
Now reporting app will read from Read replica
Create read replica in multiple regions

Consistency 
data is updated simultaneously in (standbys and replicas)

Strong Consistency
sync replication to all replicas
will be slow if u have multiple replicas/standbys

Eventual Consistency
Aysnc replication 
eg. social media posts

Read-After-Write Consistency
Insert are immediately available
update/delete are eventually consistent
eg. S3


Choosing type of DB
1) fixed schema or schema less
2) Transactional  properties (automaticity and consistency)
3) latency requirements 
4) TPS
5) how much data to store 

Relational Database
predefined schema
strong transactional capabilities

OLTP
large no. of small transactions
eg. ERP, CRM, Banking
MySQL,Oracle , 
Amazon RDS 
***  Recommended AWS Service  Aurora (based on PostgreSQL)
each table row is stored together
efficient for processing small transactions

OLAP
*** analyse petabytes of data
Eg. reporting apps, DWH, business intelligence, 
Data is consolidated from multiple transactional databases
*** Recommended AWS Service  Redshift based on PostgreSQL
*** use columnar storage
each table column is stored together
high compression
distribute data , one table in multiple cluster nodes
complex queries can be executed efficiently

Document Databases
*** data stored as set of documents (whole json)
structure data the way you application needs it
one table instead of dozens
schema-less   semi structure data
useCase: content mgmt, catalogues, user profiles
adv. horizontal scaleable to TB with ms response million of TPS
***  Recommended AWS Service  DynamoDB

*** Key-value
use simple key-value pair 
key is unique, value can be obj or simple data values
adv. horizontal scaleable to TB with ms response million of TPS
***  Recommended AWS Service  DynamoDB
useCase: shopping cart,gaming apps, v. high traffic web apps

Graph DB
store and navigate data with complex relationships
eg fraud detection , fb, social networking data 
***  Recommended AWS Service  Neptune

In Memory Databases
*** microsecond latency
storing persistent data in memory
Recommended AWS Service
*** Redis  for persistent data
Memcached for simple cache

use cases: session management, geospatial apps

DB Scenarios

***  A start up with quickly evolving tables  ElastiCache
DynamoDB

*** Transaction app need to process millions of TPS
DynamoDB

Very high consistency of data required while  processing thousands of TPS
*** RDS

Cache data from db for a web app
ElastiCache

Relational DB for analytical processing of Petabyte of data
***  RedShift

--------- Amazon RDS--------------------------------------------------------------------------

RDS is a managed service
For a managed service , you no need to worry about
setup, backup, scaling , replication and upgrade patching 
point in time restore, 
** multi AZ setup for DR
storage backed by EBS
But you can't ssh in the underlying EC2 of RDS

AWS supported Database Engine Types
* Amazon Aurora (postgreSQL + MySQL) not for free tier
PostgresSQL
MYSQL (InnoDB storage engine full support)
MariaDB (enhanced mysql for enterprises)
Oracle DB
Microsoft SQL Server

** Multi AZ deployments (standby in another AZ)

** Read Replica
Same AZ, Multi AZ, Cross Region
auto scaling storage

Automated backup
daily full backup , transaction log are backup every 5 mins
7 days retention can be increased to 35 days

Manual snapshots
manually triggered by the user
retention of backup for as long as u want

AWS Responsible 
availability (as per your configurations)
durability
scaling  (as per your configurations)
Maintenance patches 
backups 

You cannot 
** ssh in db ec2 instance or setup custom software
not allow to install db patches

You can 
manage db users
app optimisation


*  ----------- Multi AZ Deployments ------------------
Mainly used for Disaster Recovery
** Standby created in a diff AZ with one DNS name  for failover
Increase Availability
Synchronous replication
** No downtime when DB is converted to MultiAz
Not used for scaling
for patches
apply patches at standby and then switch primary with standby
** standby is automatically deleted when u delete the DB
* Read Replicas can be setup as Multi AZ for DR

* ----------------- Read Replicas ------------------------------
RR used to scale your read
it supports read-heavy database workloads 
use case: reporting, DWH
can be in same AZ , diff AZ or diff Region
App can connect to them
Create read replicas of a read replica
** use Async replication (eventually consistency ie with delay)
** Read Replicas and Manual Snapshots are Not deleted when DB is deleted , needs to delete it manually
** Must to enable automatic backups before creating read replicas
reduce replication lag by Vertical Scaling
Read replicas are used for Select only stmt
Max No of replicas
* MySQL,mariaDB,PostgresSQL, Oracle = 5
Aws Aurora = 15
SQL Server not supported read replicas

** There is Network Cost when data goes from one AZ to another AZ
DB and Read Replica  if  Within same  AZ  = Free

Services -> RSD
Create Database
standard create
Engine Type: MySQL
Template: Free tier
Instance: mysqldbInstance1 (should be unique across Region)
admin / admin123
Brustable Classes  db.t2.micro
SSD 10GB
Default AVP
public access = Yes
Security Group = rds-security-group
Initial database name: todos  (schema)

EC2 instance
Name: TestInstnace
Security Group:  my-EC2-security-group

edit  rds-security-group 
inbound : add  Custom -- my-EC2-security-group

Enable Deletion protection
can't delete DB unless mark this un-checked

End point 
mysqldbinstance1.cellrtm8cnov.ap-south-1.rds.amazonaws.com

DBvisulizer URL
jdbc:mysql://mysqldbinstance1.cellrtm8cnov.ap-south-1.rds.amazonaws.com:3306/todos?characterEncoding=latin1&useConfigs=maxPerformance

Downloaod SQL Electron
A simple and lightweight SQL client desktop/terminal
https://sqlectron.github.io/


cd C:\Users\AliImran\Box Sync\P52\My-Learning\My-AWS\Architect\in28Minutes-AWS\LABS
ssh -i "EC2-keypair.pem" ec2-user@ec2-13-233-198-190.ap-south-1.compute.amazonaws.com

sudo yum update

To install mysql client
sudo yum install mysql
msql --version

mysql --host=mysqldbinstance1.cellrtm8cnov.ap-south-1.rds.amazonaws.com --user=admin  --password todos

create table users ( id integer, username varchar(30) );
insert into users values(1, "Ranga");
select * from users;

RDS -Security and Encryption

1) At Rest Encryption
** through IAM and SG
encrypt master and read replica with AWS KMS - AEC-256 encryption
** Encryption has to be defined at launch time
* If master is not encrypted, the read replica can't be encrypted
Transparent data encryption TDE is available for Oracle and SQL server

2) In flight Encryption
SSL certificates to encrypt data to RDS  in flight
To enforce SSL
PostgresSQL : rds.force_ssl=1
MySQL : GRANT USAGE ON *.* TO 'mysqluser'@'%' REQUIRE SSL;

create a VPC private subnet
security groups to control access
option to use IAM authentication 
Enable encryption using keys from KMS
when Encryption is enabled data in database ,automated backups, read replicas and snapshots are all Encrypted
Use SSL certificates to encrypt data in flight (EC2 to RDS)

**  snapshots (backups) of un-encrypted RDS databases are un-encrypted
** snapshots (backups) of encrypted RDS databases are encrypted

To Encrypt an un-encrypted RDS database
create a snapshot of  un-encrypted  db
copy the snapshot and enable encryption for the snapshot
restore the database from encrypted snapshot
migrate application to new db and delete old db

** RDS databases are usually deployed within a private subnet
leveraging DG
** IAM policies help control who can manage AWS RDS through RDSAPI ie 
who can create db, delete db

IAM Authentication
** works with MySQL and PostgreSQL only
no need password, just a token obtained through IAM and RDS API calls
valid for 15 mins
IAM to centrally manage users instead of DB

RDS Costs

1) DB instance hours
**  2) Storage per GB per month  , you provisioned and not usage

Backups and snapshot storage
data transfer cost outside in AZ and Region, within region free data transfer


----------------------------------RDS-Amazon Aurora ------------------------------------------

** It is propriety of AWS and not open sourced
AWS cloud optimized and claim 5X performance over MySQL and 3X over Postgres
** Storage automatically grow inclemently of 10GB up to 64GB
** 15 replicas
Instance failover
** Cost is 20% less than RDS but is efficient

** it maintains 6 copies of  your data across 3 AZ
* Read Replicas can be Global 
4 copies out of 6 for writes
3 copies out of 6 for reads
*** replication + self healing + auto expanding  
Uses Cluster shared storage volumes (multi AZ storage)
it creates cluster of volumes spread across multiple AZ 
***Primary instance  read/write to cluster vol and Replicas read from cluster volume
***provides Global database options (multiple regions)
support for cross region replication
Backtrack: restore data at any point of time without using backups
** Aurora Security similar to RDS as it uses same engine
auto scaling  of storage from 10GB to 64 GB

use case:
same as RDS but with less maintenance / more flexibility / more performance

Deployment Option

Client use writer endpoint pointing to master which writes
Client use reader endpoint pointing to readers cluster (load balancing) for read

1) Single Master 
one writer and multiple readers

2)  Multi Master 
multiple writers

3) Aurora Serverless
Automated DB instantiation and auto scaling
***No need to provide the size and capacity planing
min or max to be provide and aws scale automatically
* good for infrequent, irregular and unpredictable workloads
*** pay per second and can be more cost effective

Client connect to proxy fleet (managed by Aurora)

GLOBAL  AURORA

1) Cross Region Read replicas
use for DR
simple to put in place

***2) Aurora Global Database (recommended)

One primary region (read + write)
up to 5 secondary (read-only ) regions, replication lag < 1 second
up to 6 read replicas per secondary region
*** RTO < 1 min for DR in other region

RDS -->
Standard Create
Amazon Aurora
with MYSQL compatibility
Version: 5.6.10a

Database location
Regional or Global

Templates
Dev/Test

--------------------------------------------------------------------------------------------------

Maria DB
MySQL Compatible DB

Oracle
select Edition
some has license included, some not 

SQL Server
License is included 

RDS Scaling
* Normally manual scale up to 64TB 
SQL Server up to 16TB
storage and compute typically applied during maintenance window or you can choose app-immediately

Horizontal Scaling
configure read replicas
***For Aurora (multi-master, writer with multiple  readers)

Use cloudwatch for historical data
configure cloudwatch alarms when near max capacity
slow queries  , enable enhanced monitoring
autonomic backup during backup windows in S3 , default retain 7 days, max 35 days
***Achieve RPO up to 5 mins


When to use RDS
***1) pre-defined schema
***2) where strong transactional capabilities and complex queries required

RDS is Not  recommend for 
Highly scalable massive read/write eg. millions of writes/sec  (go for DynamoDB)
Upload files using Get/PUT Rest API (use S3)
heavy customisation for DB or need to access underlying EC2 (Go for custom DB installation)

*** Migrate on-premise database to cloud database of same type
AWS Database Migration Service

Migrate data from one DB engine to other
*** use AWS Schema Conversion tool

reduce global latency and improve DR
***use multi region read replica

select subnets a RDS instance is launched into
create DB subnet groups

Add encryption to an unencrypted db instance
create DB snapshot
encrypt the snapshot using keys in KMS
create database from encrypted snapshot

Billed if DB is stopped 
*** Only for storage, IOPS , backups and snapshots
Not billed for DB instance hours

Need RDS for an year, reduce cost 
*** use RDS reserved instances

Efficiently manage DB connections
*** use AWS RDS Proxy
sits b/w client app (including lambda) and RDS

------------------AWS ElasticCache-------------------------------------------

Managed Service
* Highly scalable and low latency in-memory key value data store
*sub millisecond latency
** u can store in memory data in EC
as a distributed caching solution
** must provision an EC2 instance type
WRITE scaling using sharding
READ scaling using Read Replicas
* Multi AZ with failover 
*point in time restore feature
* can't use SQL

 Cash Miss
 get data from db

 Cash Hit
 found data in the elastic cache
 
 User Session Store
 user logs into a application 
 application writes session data into ElasticCache
 if user hits another appl instance then its session is retrieved from EC
 
 All caches in ElasticCahce supports SSL in flight encryption
 *** Don't support IAM authentication
 Redis AUTH
  can you enhance the security of your Redis cache to force users to enter a password?
 can set password/token when u create a redis cluster
 
 memcached supports SASL based authentication
 
 
** Redis and Memcached are two Custer engine for ElasticCache
 
1) Redis
in memory persistence data store
* Multi AZ deployments with automatic failover
* durability using AOF persistence
*  supports backup (in S3) and restore
* can be used as database
can schedule snapshots
configure backup windows
encryption at rest  (KMS) and in transit 
use cases: caching,session store,chat  messaging, geopolitical apps, queues
shard = collection of one or more nodes (where a portion of ur data is available)
One node act as read/write Primary
Other nodes act as read replicas (up to 5 read replicas)

In case of Failure:
Primary node is replaced
if Multi-AZ replication group is enabled, read replica is promoted to primary
* publish subscribe messing  (act as a  message broker)
read replicas and failover support
encryption support

2) Memcached
pure caching sol
distributed 
Multi Node for partitioning of data (sharding)
** Non persistent cache
multi threaded architecture
simple key value storage
** ideal for front end for data stores like RDS and DynamoDB
can be used as a Transient  session store
create up to 20 cache nodes
use Auto discovery to discover cache
Low maintenance simple caching solution
Easy auto scaling

Limitations
** No backup or  restore supported
** No encryption or replication or snapshot
** when node fails, all data in that node is lost 
reduce impact of failure by using Large no. of small small nodes

Cluster Engine : Memcached


Patterns for ElasticCache

1) Lazy Loading
all read data is cached, data can become stale in cache

2) Write Through
add or update data in cache when written to DB (No stale data)

3) Session Store
store temporary session data in cache using TTL feature
Storing Session Data in ElastiCache is a common pattern to ensuring different instances can retrieve your user's state if needed. 

Multi AZ ensure High Availability
Global Databases allow you to have cross region replication

------------------------ AWS DynamoDB --------------------------------------------------------

Fully managed , HA, fast, scalable DISTRIBUTED  NoSQL DB for any scale
** schema less
** 3 replica in a single region
** NoSQL key value and document based
Don't worry about scaling , availability or durability
it automatically partitioned data as it grows across multiple nodes
single-digit millisecond latency at any scale.
millions of requests per seconds and trillions of rows , 100s TB of storage
Low cost and auto scaling capability
No need to create  a Database
** here create a table directly and configure RCU and WCU Read capacity unit
each table can have infinite items (rows)
each item has attributes (columns)
max item size = 400KB
provides a expensive serverless  mode
** application which required milli second latency but at v high scale
** Enables event driven programming with DynamoDB streams
Throughput can be extended temporary using "burst credits"
"ProvisionedThroughputException" when burst credits are empty so it is advised to do exponential back-off retry
* can only query on primary key, sort key or indexes
*** can replace ElasticCache as key/value store for storing session data


Hierarchy 
Table ->Items -> attributes (key value pairs)
Mandatory Primary Key
Max 400KB item size

DyamoDB 
table name: todo
Primary key : id  (mandatory) used to distribute the data across different partitions
Add sort key 

TTL time to live 
expiry of record

Manage Stream
ie upon any event execute stream,

** DynamoDB NewFeatures

1) Supports Transactions across multiple tables
include up to 10 unique items or up to 4 MB of data

2) On Demand
No capacity planing needed, scale automatically
**  2.5X more expensive than provisioned capacity
useful in case of un predictable spikes or application is v low throughput

Security 
VPC endpoints available to access DynamoDB without internet
Access controlled by IAM 
Encryption at rest KMS
Encryption in Transit SSL/TSL

Backup and restore
** point in time restore like RDS
No performance impact

Migrate to Dynamo DB using DMS (from MongoDB Oracle,MySWL,S3)
can launch a local DynamoDB on your computer for development purpose

scan is expensive operation which query is efficient
based on specific metric , you can create alarms

if you need to create the query by a column other than Id then create an index of that column

keys in Json is case sensitive 
eg. Address and address will create two columns#

{
"Address": "Lahore",
"id": "001",
"name": "Ali"
}

Create Index
Key : Address
Name: Address-index

** Global tables
**  enable you to use DynamoDB as fully managed , multi region, multi-master database
to create global table , 
** Must enabled dynamoDB streams
Active Active replication , in many regions
useful for DR 
CRUD in one Global table automatically reflected in to other table of different region and vice versa

Triggers 
** connect DynamoDB streams to Lambda functions whenever an item in the table is modified , a new
stream record is written, which in turn invoke the lambda function

Access control
helpful for direct database access by mobile apps, web identify federation allows your
mobile apps to use identity providers as Login with Facebook, Google

** partition key is mandatory for search
can't search using only sort key

partition key + sort key = composite pk

DynamoDB Indexes

Local Secondary Index
Same partition key  as of Primary Key but different sort key
Should be created at the table creation

Global Secondary Index
Partition and sort keys are diff from Primary Key
Can be added or removed at any point in time
stored separately from original table

Query VS Scan

Query
search using partition key (PK or Index) and a distinct value to search
optional: sort key and filters
results are sorted by PK
Max 1 MB result returned

SCAN
read every item in a table
** expensive 
return all attributes by default
support paging above 1MB
Filter items using expressions

Consistency Levels
** Eventually consistent (1 sec lag by default)
Request for strongly consistent reads
Set ConsistentRead to true (slow and expensive)
supports Transactions but cost will be twice

Read/Write Capacity Modes

** **  In DynamoDB, strongly consistent reads are expensive than eventually consistent reads

Provisioned
** Recommended
** provision read and write capacity
dynamically adjustable
unused capacity can be used in burst(in case of spike)
** Billed for provisioned capacity  irrespective of whether you used it or not

On Demand
** Truly serverless and expensive
For unknown workloads or traffic with Huge spikes
used when workloads are really spiky causing low utilization of provisioned capacity or usage is very low 

** Dynamo DB RCU and WRC
Capacity used depends on size of item, read consistency , transnational etc

Read Capacity Unit : throughput for reads
1 capacity unit to read 4KB or smaller  = 1 RCU

Write Capacity Unit : throughput for writes

1 capacity unit to write 1KB or smaller  = 1 WCU
twice the capacity for strong consistent or transactional request

** On Demand RCU is 8 times the cost of Provisioned RCU

Performance Monitoring
use cloudwatch
alerts on RCU,WCU and throttle request

Migration from RDS or MongoDB to DynamoDB
AWS Migration Service
** Enable point-in-time recovery (35 days)
** use Time to Live TTL  to automatically expire items

IAM and Encryption
Server side encryption with KMS keys is Always enabled (automatically encrypt tables, streams and backups)

Client Side encryption
DynamoDB Encryption Client

Use IAM roles to provide EC2 instances or AWS services access to DynamoDB tables
predefined polices 
AmazonDynamoDBReadOnlyAccess
AmazonDynamoDBFullccess
Fine-grained control at the individual item level

DynaymoDB
** milli sec latency with millions TPS but lower consistency
Rest API, SDK, CLI
Difficult to run complex queries
** No upper limit

RDS
** stronger consistency and transactional capabilities
SQL Queries
** Good to run complex queries
** upper Limit 64TB

-----------------** DynamoDB Accelerator (DAX)----------------------------------------------------------------------------

** In memory cache for DynamoDB
microsecond response time

Applications --> DAX -->. DynamoDB

Few changes needed to connect to DAX
can reduce your costs by saving our read capacity units (lambda reads from DAX rather than hitting DynamoDB)
solves Hot key problem (too many reads)
*** 5 mins TTL
up to 10 cluster nodes
Multi AZ min 3 
Secure , encryption at rest via KMS,VPC,IAM,cloudTrail

Not Recommanded
if u need strongly consistent reads
application is write intensive with very few reads

DynamoDB -> DAX 
create cluster
*** encryption is recommended

-------------------** DynamoDB Streams--------------------------------------------------------------------------

changes in DynamoDB (Create,update,delete) can end up in a DynamoDB stream
** each event from DynamoDB  (in a time sequenced order) is buffered  in a stream near real-time
This  stream can be read from AWS lambda 
can be enabled/disabled
* could implement cross region replication using streams
* streams has 24 hrs of data retention

EC2 -> DynamoDB -> DynamoDB Streams -> Lambda -> SNS
use case: send email when user registered
streams allow iteration through records (last 24 hrs) as batch

--------------------------------------------------------------------------------------------
S3 is a key/value store for objects like a DB
Great for big objects (5 TB) and not for small objects
serverless
--------------------------------------------Neptune----------------------------------------------------------

Fully managed Graph database
High relationship data 
social networking , Wikipedia
HA across 3 AZ with 15 read replica
point in time recovery with continuous backup to S3
IAM, KMS and Https support

 ----------------------Redshift--------------------------

Redshift is a petabyte-scale distributed data ware house  
*based on PostgresSQL
*  Redshift is a relational database
*OLAP 
10X better performance than other DW
pay as you go based on instances provisioned
from 1 node to 128 nodes, up to 160GB per node

* Redshift spectrum: perform queries directly against S3 (but not server less like Athena)
* Redshift Enhanced VPC routing, copy/unload goes through VPC and not internet

*1) MPP massive parallel processing
storage and processing b/w multiple nodes

*2) Columnar data storage

*3) High data compression
ie city column

A single row data might be stored across multiple nodes
* A query to redshift leader  node is distributed to multiple compute nodes

start with a single node configuration and scale it to multi node 
add / remove nodes dynamically
used mainly for ETL and BI cases
high performance analysis and reporting of large dataset

** supports standard SQL
Integration with data loading, reporting , reporting , Miningsby 
Its a managed service
High available and durability
* automatic replication (3 copies of data)
* automatic backup (S3 , default retention 1 day, max 35 day)
automatic recovery from any node failure
snapshots are point in time backup of your cluster stored in S3
increment , only store what has changed
can restore a snapshots into a new cluster
manual snapshot retained until u delete it
** Configure Redshift to automatically copy snapshot of a cluster to another Region

Redshift cluster

*Leader node
received sql queries and distributed to compute nodes

* Compute Nodes
can be in multiple AZ
no direct access to leader nodes

2) sort keys
* data is stored in sorted order using sort key
increase efficiency of your queries
join columns with other tables
timestamp column if u use most recent data  frequently

Aim is to distribute data equal across nodes and minimize data movement during query execution

default strategy= EVEN
KEY = based on values of one column
ALL = entire table on all node ie lookup table

Loading data in Redshift

Simple
SQL insert queries using ODBC and JDBC

Efficient
*  Redshift COPY command to load data from S3,DynamoDB, EMR

Data Pipelines
Load using AWS Data Pipeline a managed service

On premise data
* user Storage gateway or import/export data into S3 
COPY data from S3

Other databases
AWS Data Migration Service
RDS,DynamoDB

Recommendation
prefer COPY over INSERT for bulk operations as COPY is parallel and INSERT is sequential
prefer COPY from multiple files. split into multiple small input files

Managing Redshift workload (WLM)
used to categorize your queries 
create queues and put  queries in it

Redshift Security

integrates with AWS KMS or AWS Cloud HSM
It users 4-tier approach, key based architecture for encryption
1) master key (choose keys in KMS)
2) cluster encryption key (CEK)
3) database encryption key (DEK) 
4)  data encryption key

IAM to manage user permission for cluster operations
grant permissions on a cluster basis instead of per table basis
can add new columns using alter but can't alter existing columns
sql operations are logged against system tables or download to S3
monitor performance and queries with cloud watch and redshift web console
when deleting a redshift cluster, take a final snapshot to S3

Console -> Redshift
fast simple effective data warehouse 

** Redshift Spectrum
Run sql queries against datasets in S3 Without loading it
*Query is then submitted to thousands of Redshift Spectrum Nodes
** Must have a Redshift cluster available to start unlike Athena
Avro,parquet, csv,json formats supported
Redshift spectrum makes use of metadata to query from S3
scale storage and compute independently
** Eliminate expensive data transfer from S3 to data warehousing solutions (cost effective) 
Integrates with AWS Athena
Query against EMR

--------------------AWS Elasticsearch-----------------------------------------------------------------------------------------------------------

Managed service around Elasticsearch
support ELK stack
Elasticsearch
logstach to inject data
Kibana for dashboard visualization
use case: 
fast search
app/server  monitoring get intelligence from you logs

eg. In DynamoDB we only search by PK or indexes
With ElastiSearch, you can search any field , even partial matches
It has usage for Big data applications as well
you can provision cluster of instances
Can integrate with Kinesis Data Firehose, AWS IOT, CloudWatch logs

---IIMP--------------***   RDS for Solution Architect w.rt  well Architected 5 pillars-------------------------------------------------------------------------------

1) Operations
small downtime when failover happ
scaling in read replicas/ec2 instance
DynamoDB:No ops needed, auto scaling and serverless
S3: No operation required
Athena: serverless
Redshift: same as of RDS
Neptune: same as of RDS
ElasticSearch:  same as of RDS

2) Security
OS security by AWS
we will do setting KMS SmG,IAM polices , SSL
ElastiCache: use Redis Auth 
DynamoDB:IAM,KMS,SSL
S3: IAM,Bucket Policy,ACL,Encryption
Athena: IAM + S3 security
Redshift: same as of RDS
Neptune: same as of RDS + IAM
ElasticSearch: Cognito, IAM,VPC,KMS, SSL

3) Reliability
Multi AZ , failover in case of failure
Aurora: Serverless 
ElastiCache: Multi AZ,Clustering
DynamoDB:Multi AZ,Backup
S3: high durability and availability , multi AZ and CRR
Athena: managed service, use Presto engine, High available
Redshift: HA and Auto healing features
Neptune: Multi AZ, Clustering
ElasticSearch: multi AZ, clustering

4) Performance
depends on EC2 instance type, EBS volume type 
ability to Read replica
Doesn't auto scale 
Aurora: 5X faster up to 15 Read Replica
ElastiCache: sub millisecond, in memory, read replicas for sharding
DynamoDB: single digit millisecond, DAX for caching reads
S3: scales to thousands of read/writes, Transfer acceleration , multi part for big files
Athena:query scale based on size
Redshift: 10X faster than other DWH solutions, support Compressions
Neptune: best for graph clustering 
ElasticSearch: open source, petabyte scale

5) Cost
Pay per hour based on EC2 and EBS
Aurora: pay per hour based on EC2, cost less than enterprise db like Oracle
ElastiCache:  Pay per hour based on EC2 and EBS
DynamoDB: Pay per capacity and storage unit, no need to guess capacity in advance
S3: pay per storage,network cost , no. of requests
Athena:pay per query/per TB data scanned, serverless
Redshift: pay per node provisioned, 1/10 of cost vs other DWS
Neptune: pay per node as RDS
ElasticSearch: pay per node as RDS

