-----------------------------------------------S3 Fundamentals--------------------------------------------------------------------------------------
Simple Storage Service
inexpensive 
store large objects with key value approach
also called Object StorageProvides REST API 
unlimited storage
objects are replicated in a single region across  multiple AZ

Buckets (directories) are fundamental container in S3

S3 -> Create Bucket
bucket name should be using  across AWS accounts (unique)
no space and special or upper case char in Name and become part of object URL ,IP
* must start with lower case letter or number
enabling object lock automatically enables Bucket versioning 
* S3 is a global service 
however a bucket is created in a region
every object is access using key value pair
* max  size of an object in  a bucket is 5 TB
must use multi part upload for uploading more than 5GB
No hierarchy of buckets,sub buckets or folders

key is full path
keys is composed of Prefix + Object Name
s3://my-bucket/myFolder1/fileName1.txt

key = myFolder1/fileName1.txt

Objects are files 

S3 --> Create Bucket

Name:
my-s3-bucket-1

Select Region 
as bucket is region specific close to you

Go to that Bucket

Upload a folder

Storage Class
Standard

2030
10
course1.jpg
course2.jpg

key 2030/10/course1.jpg    value is image 1
key 2030/10/course2.jpg    value is image 2

PATH
http://bucketName/key
s3://my-s3-bucket-1/2030/10/course2.jpg

OBJECT URL
https://my-s3-bucket-1.s3-eu-west-1.amazonaws.com/2030/10/course2.png


Bucket Level Property

VERSIONING 
Versioning is at Bucket level must be enabled first
increment the version by same key
any file that is not versioned prior to enable versioning will have version "null"
But if suspend versioning the previous version files will not delete and new vision will not create
But if we delete a specific versioned then it will delete permanently

my-s3-bucket-1 -> Properties -> Versioning

Enable Versioning


All old objects will have a version of null
We can't turn off the versioning once set

WARNING! BILLING ALERT! Do NOT Enable Access Logging For Your Buckets
Enable Logging 
Target Bucket my-s3-bucket-1
prefix logs

Permission
S3 log delivery group

Creating a Public website with S3

Static Web site hosting
Index document: index.html
on root of the bucket 
1) select all file 
C:\Users\AliImran\Box Sync\P52\My-Learning\My-AWS\Architect\in28Minutes-AWS\course-presentation-and-downloads\s3
Permissions

2) Edit  Block public access

3) Bucket policy ()resource based policy , give cross account access)

Documentation https://docs.aws.amazon.com/AmazonS3/latest/dev/using-iam-policies.html
Granting Read-Only Permission to an Anonymous User

copy readonly access and put ur bucket name

{
"Version":"2012-10-17",
"Statement":[
{
"Sid":"PublicRead",
"Effect":"Allow",
"Principal": "*",
"Action":["s3:GetObject","s3:GetObjectVersion"],
"Resource":["arn:aws:s3:::my-s3-bucket-1/*"]
}
]
}


https://my-s3-bucket-1.s3-eu-west-1.amazonaws.com/index.html

Object level Logging and Encryption
using cloud trail


------------- S3 ENCRYPTION-------------------------
None 

Four methods for encrypting objects in S3

1)  SSE_S3:
server side encryption
Encrypt S3 objects using keys handled & managed by AWS
AES-256 encryption type
Must set header:  "x-amz-server-side -encryption":"AES256"
example:
pass http/https request with above header to S3
S3 encrypts object using its own managed data key and put  the encrypted obj in the bucket

2) SSE-KMS (key management service)
server side encryption
Leverage AWS key management service (AWS-KMS) to manage encryption keys
Must set header:  "x-amz-server-side -encryption":"aws:kms"
** KMS uses CMK Customer master key for encryption 
** With SSE-KMS you let AWS manage the encryption keys but you have full control of the key rotation policy

3) SSE-C
manage your own encryption keys
server side encryption
using data keys fully managed by customer outside of AWS
S3 doesn't store that encryption key
** Https is mandatory
encryption key must be provided in the header for each request
Here you have full control over the encryption keys, and let AWS do the encryption

4) Client Side Encryption
encrypt the data before uploading to S3
client libraries like S3 Encryption client can be used
customer fully manages the keys and encryption cycle

Encryption in transit/flight is SSL/TLS
S3 provides both http/https  but https is recommend

using Bucket Properties we can  set Default encryption
it is being applied event it is set none using upload a file

1)None
2) AEC-256
3) AWS-KMS

S3 SECURITY

1) User based
IAM policies, which API should be allowed for a specific user from IAM

2) Resource Based
Bucket policies - bucket wide rules from S3 console- allow cross account
JSON based 
Resources: buckets and objects
Action: set of API to allow or deny
Effect : Allow / Deny
eg. 
Grant public access to the bucket
Force objects to be encrypted at upload
rant access to another account (Cross account)

Object Access - ACL - fine grain
Bucket Access - ACL - less common

** an IAM principal can access S3 object if 
the user IAM permission allow it 
OR
The resource policy Allows 
AND 
there's no explicit DENY

***Explicit DENY in an IAM policy will take precedence over a bucket policy permission


Support VPC endpoint 
S3 access logs can be stored in other S3 bucket
API calls can be logged ni AWS cloudTrail
MFA for delete objects
Pre-singed URLS for a limited time

Bucket --> Management --> Bucket Policy
We acan leverage S3 bucket policy generator
https://awspolicygen.s3.amazonaws.com/policygen.html

* Also Block public access to the bucket at the account level 

*** Object Lock  (same asa Glacier Vault lock)
prevents object from being deleted fpr a specified amount of time 
Enable only at the time of creation bucket in advance setting, also versioning enabled
after that we can't delete objects from bucket eg. for regulatory constrains
adopt WORM (write once read many) model

**MFA Delete forces users to use MFA tokens before deleting objects. It's an extra level of security to prevent accidental deletes
** all your files in S3 to be encrypted by default
Enable Default encryption on S3
** S3 Access Logs log all the requests made to buckets, and Athena can then be used to run serverless analytics on top of the logs files
**  automate the transition of S3 objects between their different tiers?
use S3 LifeCycle Rules

S3 Tags
used for automation ,polices cost tracking
key values

S3 WebSite
can host static websites 

<bucket-name>.s3-website-<aws-region>.amazonaws.com

Static website hosting
provides index.html and errpr.html
if u get 403 forbidden error, make sure the bucket policy allows public reads
1) Disable Block access 
2) Create a bucket policy

Transfer acceleration
to improve speed of data transfer 
not free

Requester Pays
In general, bucket owners pay for all Amazon S3 storage and data transfer costs associated with their buckets. 
With Requester Pays buckets
- The requester instead of the bucket owner pays the cost of the request and the data download from the bucket
The bucket owner always pays the cost of storing data.
when this is enabled
anonymous access to the bucket is disabled

Events
Notifications to Lambda fns

Event Sources
new obj created,removal
RRS object lost events
replication across events

Events Destination
SNS Topic
SQS queue
Lambda Fn


Versioning CANNOT be configured at an individual object level?
S3 Life Cycle Configuration allows you to move files between different S3 Storage Classes?    
ACLs are primarily used to grant permissions to the public and other AWS accounts


Create a lambda function
S3NotificationLambda

Create event in S3 Bucket  in my-s3-bucket-1
S3NotificationEvent
PUT, POST
Send to Lambda Function
lambda S3NotificationLambda

Now add a file to the bucket and 

Select lambda Function 
View logs in CloudWatch

S3 Prefix
search for keys starting with a certain prefix
URL?prefix=2030/10
supported by rest API,aws cli,aws sdk, conle
Used in IAM and Bucket polices to restrict access to a specific files or group of files

http://s3.amazonaws.com/my-s3-bucket-1?prefix=2030/10
getting error

http://s3.amazonaws.com/my-s3-bucket-1/index.html


Now S3-> Permissions --> Access Control Listener--> Public Access --> List Object 

Prefix is the substr at the start of the key

it is accessible using below url and displayed the XML
http://my-s3-bucket-1.s3.amazonaws.com/
or 

http://my-s3-bucket-1.s3.amazonaws.com/?prefix=2030/10

CORS
cross origin resource sharing
origin is a scheme consisted of protocol + host+ port
its a web browser based mechanism to allow request to other origins while visiting  main origion

same origin
http://example.com/app1 
http://example.com/app2

Different origin
http://example.com
http://otherexample.com

The request won't fullfiled unless the other origin allows for the request using
CORS headers (ex:Access-Control-Allow)

S3 CORS
if a client does a cross-origin request to our S3 bucket with enabled website , we 
need to enable correct CORS headers
** You can allow for a specific origin or use * fro all origins
CORS headers must be defined at the other bucket (tow whome first origin want to send the request )
when it allowed access for CORS then browser can be able to request for that site


Create a new bucket 
same region
make it public
Permission -> add bucket policy
enable static web shosting
Permission -> CORS configuration
add the configuration file with full URL of the origion in AllowedOrigin


** Block Public Access is at Highest level than access control and bucket policy
Bucket level is a policy at bucket level

Access Control List is at bucket and object levels

Use Object ACL when bucket owner is not the object owner
need diff permissions for different objects in the same bucket

IMP
** Bucket / Object ACL don't have conditions but polices can have

ACL are primarily used to grant permissions to public or other user accounts

-------------S3 MFA -Delete ---------------------------------------------------------------
To use MFA delete, enable versioning on S3

You will need MFA to restrict
* permanently delete an object version
* suspend versioning on the bucket

You won't need MFA to
enable versioning
list delete versions

* Only the bucket owner (root account) can enable/disable MFA delete
* MFA-Delete currently can only be enabled using CLI

-------------S3 Access Logs -----
for audit purpose , any request made to S3 from any account autorized or denied 
will be loged into another S3 bucket
* Don't set your logging bucket to be the monitoring bucket, (infinite loging loop)

create bucket 
S3AccessLog

create bucket 
mySampleBucket
Properties -> Enable Logging -> Target Bucket -> S3AccessLog

------------------------------------------------------------------------------------------------------
S3 Default encryption vs Bucket Policies

old way to enable default encryption was to use bucket policy and refuse any 
http command without proper header
But
New way is to use "default encryption" option in S3
** Bucket Policies are evaluated before "default encryption"

--------------------------S3 Storage Classes-----------------------
wide variety of data
huge variations in access pattern
S3 storage classes help to optimise your cost  while meeting access time needs
durability 11 9's

when we upload the file in Bucket t, then storage classes option comes
we can change storage class of a object


Standard S3 - General Purpose
* Frequently access data
* High Durability 
* data replicated in at least 3 AZs
Encryption = Optional
per GB cost = 0.025
use cases: 
Big data analytics,mobile and gaming , content sharing


S3 Standard-1A
* long lived, infrequently access data  (eg backups for DR)
* data replicated in at least 3 AZs
Encryption = Optional
per GB cost = 0.018
lower than S3 standard
use cases: 
data store for DR and backups
** Minimum storage duration is 30 days

S3 One Zone-1A
Non Critical data
long lived, infrequently access data  (data that can be easily generated again)
* data replicated in only 1 AZs
Encryption = Optional
per GB cost = 0.0144
use cases:
storing secondary backup copies of on-premise data or storing data that you can easily re create
eg. thumb nails from image 


S3- Intelligent-Tiering
same low latency and high throughput performance as of S3 standard
small monthly  monitoring and auto Tiering fee
automatically moves objects b/w two access tiers based on changing access patterns
Long lived data with changing or unknown access 
multiple zone for resiliency
AWS choose storage classes standard or standard1A
Encryption = Optional
** Minimum storage duration is 30 days

Amazon Glacier
v low cost 
10s of years
alternative to on-premise magnetic tape
Each item in Glacier is called "Archive" file upto 40TB
Archives are stored in "Vaults"
Archive data with retrieval times ranging from minutes to hour
Encryption = Mandatory
0.005

** 3 retrieval options
1) Expedited (1 to 5 mins)
2) Standard (3 to 5 hours)
3) Bulk (5 to 12 hours)
** Minimum storage duration is 90 days

Glacier Deep Archive
Archive data with that  rarely retrieval times ranging from hours  to days
Encryption = Mandatory
0.002

1) Standard (12 hours)
2) Bulk (48 hours)
** Minimum storage duration is 180 days


For durability All S3 classes are 11 9s

You can switch storage class of an object in a  bucket or during file upload

Life Cycle Management

how can you save costs and move files automatically between storage classes 
sol: S3 Life-cycle Configurations

TRANSITION ACTIONS
one storage class to another
defines when objects are transitioned to another storage class
ie more objs to Standard IA class 60 days after creation
Move to Glacier for archiving after 6 months

EXPIRATION ACTIONS
delete obj eg after one month
can be used to delete old versions of files if versioning is enabled
ca be used to delete incomplete multip part uplods
rules can be created for a certain prefix  ie /mp3/*

Storage Classes -> Management -> Lifecycle Rules
Life Cycle Rules is NOT FREE
S3 Cross-Region Replication is NOT FREE

------- S3 Replication  --------------- 
*Can  be in same region and multiple region
Could be cross account
Access to destination is provided using IAM policy
*Versioning should be enabled on BOTH source and destination
Only new objects are replicated 

CRR Cross region replication
compliance , lower latency access , replication across accounts

SRR Same region replication
log aggregation , live replication from prod to dev accounts

* after activating S3 replication, only new objects replicated
* If you delete with/without version id , delete marker will NOT replicated
* No Channing ie B1-B2-B3 then not B1 to B2 automatically

Crate two buckets B1 and B2 in diff regions
Enable versioning in both buckets
in B1- Properties -> Replication - Add rule - select B2 as destination
then select a new Role 

Management -> Replication -> Add Rule

Object Level Configurations
select OBJECT
Properties
Can override  Storage Class, Encryption,metadata, tags
Object lock is cannot be enabled at Object level 
Permissions can be changed Object ACls 

** S3 CONSISTENCY MODEL
S3 is distributed and it maintains multiple copies of your data in a regions to ensure durability 

1) READ AFTER WRITE FOR PUTS of new object
means when u create a new objects, it is immediately available
PUT 200 --> GET 200

2) EVENTUAL CONSITENCY for Overwrite DELETES and PUTS
means no guarantee, you might get a previous version of data immediately after an object is updated 
PUT 200 --> PUT 200 - GET 200 (might get older version)
If we delete an object , we might still be available to retrieve it for a short time 
DELETE 200 --> GET 200 

** There is no way to request or API "strong consistency"
so u need to wait a while after updating an object 

-----------------------------------------------------------
S3 Pre signed URL
Grant time limited permission (few hours to 7 days) to download objects
Avoid web site scraping and unintended access
using AWS SDK API
input 
security credentials, bucket name,object key , HTTP method , expiration date time 
output 
pre signed url

for downloads : easy can use CLI
for upload : harder, must use SDK

default valid for 1 hour
can change it TIME_BY_SECONDS argument
users given a presiged URL inherits the permissions of the person who generated the URL

example:
allow only certification permium users to download video from S3
changing list of users by generating dynamic urls

aws s3 presign help
aws configure set default.s3.signature_version s3v4
aws s3 presign s3://mybucket/myfile.jpg --expires-in 300 --region eu-west-1

this results in a URL which is accessible in 5 minutes
-------------------------------------------------------------
S3 Access Points

simplify bucket policy configuration
access specific vpc to a specific bucket
create application specific access points with an application specifi policy

for App1 we can set different kind of action and for App2 diff action on the same bucket

Bucket -- Access points
one for each application to access

S3 Cost Factors
Cost of Retrieval Charge Per GB
Monthly Tiering (only for Intelligent Tiering)
Data Transfer Fee 

Free
Data  Transfer into S3 
Data  Transfer from  S3 to CloudFront
Data  Transfer from  S3 to Services in the same region (EC2, lambda Fn in same region)

S3 Security Scenarios

Prevent Object from being deleted or overwritten
Use S3 Object Lock

Protect against accidental deletion
Use  Versioning

Avoid Content Scraping
Pre-Signed URL also called Query String Authentication

Enable Cross Domain request to S3 hosted web site
Use CORS

S3 Cost Scenarios

1) Reduce cost 
use proper storage classes  and configure life-cycle management

2)  analyse storage access patterns and decide  right SC
use Intelligent Tiering 
use storage class analysis report

3) Move data automatically b/w SC
use lifecycle rules

4) Remove objects from bucket after a specified time period
use life cycle rules and configure expiration policy


------------ S3 PERFORMANCE SCENARIOS------------------

S3 is serverless
recommended for large objects

1) Improve S3 bucket performance
S3 automatically scales to high request rates, latency 100-200 ms


Use S3 prefix
supports up to 3500 request per second to add data (PUT/COPY/POST/DELETE)
supports up to 5500 request per second per prefix in a bucket to retrieve data

There is no limits to the number of prefixes in a bucket
bucket/folder1/sub1/file  = object path
folder1/sub1/ = prefix

2) Upload large objects to S3
use multipart upload API
quick recovery from a network issue
pause and resume object upload
* recommend for files > 100MB and Must for files > 5GB

3 )Get some part of the object

Use Byte-Range_fetches

4) EC2 (Region A) accessing S3 bucket(region B)
Not recommend , reduce network latency and data transfer cost


5) How make user pay for S3 storage
Requester pays

6) Create inventory of S3 objects 
use S3 inventory report

7) Need S3 bucket access logs
enable S3 Server Access logs (default:off)


8) change object metadata or tags or acl or invoke lambda functions for billions of objects in S3
Generate S3 inventory report and Perform S3 Batch operations using it

9)  Need S3 Object bucket Access logs
Enable S3 Server Access Logs

S3 KMS limitation
if u use SSE-KMS you may be impacted by KMS Limits
when  u upload it calls GenerateDataKey KMS API
when u download it calls Decrypt KMS API

Count towards KMS quota per second 
(5500,10000,30000 req/sec based on region)
can't request quota increase for KMS

TRANSFER ACCELERATION (fast/secure file transfer to/from bucket )
* only for upload
increase transfer speed by transferring file to edge location
eg
File is US --fast using public www --> Edge Loc at US ---> fast private AWS -- S3 bucket in Australia

S3 BYTE RANGE FETCHES
parallelize GETs by requesting specific byte ranges 
better resilience in case of failure
can be used to speed up downloads from S3
can be used to retrience only partial data ie first 50 bytes


S3 Notifications
process streaming data
S3:ObjectCreated, S3:ObjectRemoved
Object name filtering possible (*.jpg)
use case: generate thumbnails of images uploaded in S3
eg. take image and customise it with lambda function for different devices in S3
S3 to Lambda is using S3 notification
SNS,SQS or trigger functions on S3 objects
at bucket levels
using prefix and sufix
cost effective 
*** If you want to ensure that an event notification is sent for every successful write, you can enable
versioning on your bucket
Go to events -> 
name = SQS
All object create
send to : SQS/lambda/SNS   (queue should be in same region as of bucket)
Go to SQS queue --> Permission -- Allow 

------------------------------------------------------------------------------------------------------
S3 Glacier is a separate Service 
Amazon S3 Glacier is an extremely low-cost storage service that provides secure, durable, 
and flexible storage for data backup and archival
As  a replacement for magnetic tapes
High durability 11 9's
High security as encryption is must
Cannot upload objects to glacier using management console but using 
REST API, AWS CLI, AWS SDK

S3
object are stored in buckets
object keys are user defined
allow uploading new content to the objects ie object can be modified
object size can be up to 5TB
operations at bucket and object level supported
Encryption is optional
WORM write once read many times = Enable object lock policy
can immediately  down data , synchronous 

S3 Glacier  (is a regional service)
archives are stored in Vaults
object keys are system defined
can't be updated , best for regularity compliance
object size can be up to 40TB
only vault level operations supported
Encryption is mandatory
WORM write once read many times = Enable Vault lock policyAsynchronous 2 steps 
1) initiate a archive retrieve
2) down the archive

Reduce Cost
can reduce cost by optionally specify a range or portion of archive
Requesting longer access time

Expedited 		1-5 mins  (make sure you have provisioned capacity available)
Standard   		3-5 hours
Bulk retrieval 	5-12 hours


Console --> S3 Glacier --> Create Vault
my-vault
Enable Notification
SNS topic 

------------------------AWS Athena ------------------------
Serverless service to perform analytics directly against S3 files
** Used to analyse data on S3 with out loading into any DB and directly from S3
Query Engine over S3
uses SQL language 
Has JDBC/ODBC driver
charged per query and of data scanned
support scv,json,avro and parquet
pay per query 

use case:
reporting , analysing , VPC flow logs, ELB logs , cloudTrail logs

Athena -> Query Editor
query result location = s3://aws-results
New Query -> 
create database s3_access_logs_db;
create table myTable on location myBcuke name

