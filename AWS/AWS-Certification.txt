----------------------------------------------------------------------------------
Must do 

AWS Exam Readiness
https://www.aws.training/Details/Curriculum?id=20685

AWS Certified Solutions Architect - Associate (SAA-C02): Introduction
https://learning.oreilly.com/videos/aws-certified-solutions/9780136721246/9780136721246-ACS2_00_00_00


White paper and FAQs are must

What is AWS Well-Architected Tool?
https://docs.aws.amazon.com/wellarchitected/latest/userguide/intro.html


cheat sheet
https://tutorialsdojo.com/links-to-all-aws-cheat-sheets/

Find Qualifier  keywords
Elimination Technique
Time Management




-----------------------------------------------------------------------------------

https://202041525381.signin.aws.amazon.com/console


------------------------------------------------------------------------
Questions
EC2 Custom AMI vs Custom Template

------------------------------------------------------------------------------------------------------------------------------------------
https://yourlearning.ibm.com/activity/URL-AA49C4823B02

https://smarter-gbs.yourlearning.ibm.com/#/cpt/roadmap/1725


--------AWS Certification Material---------------------------
Amazon Web Services (AWS) Certified - 4 Certifications!

Exam Readiness: AWS Certified Solutions Architect – Associate (Digital) Free
https://www.aws.training/Details/Curriculum?id=20685

Exam Readiness: AWS Certified Developer – Associate (Digital)
https://www.aws.training/Details/Curriculum?id=19185

Free AWS Material
https://digitalcloud.training/amazon-aws-free-certification-training-solutions-architect/

https://aws.amazon.com/certification/certification-prep/


AWS Free Digital Training
https://www.aws.training/LearningLibrary?filters=language%3A1&filters=digital%3A1&tab=view_all


Neal Davis
https://digitalcloud.training/aws-csaa-hands-on-labs-downloads/

AWS Cloud Practitioner Essentials (Second Edition) Prerequsite
Good https://www.aws.training/Details/Curriculum?id=27076


download 
AWS Certified Solutions Architect Official Study Guide


---------AWS Badges---------------------------------AWS Certified Solutions Architect - Associate ----COURSE (skillsoft)----------------------------------------------------------------------
https://learn.percipio.com/channels/783c9bb0-2b77-11e7-9d24-490aed2acd57

1) AWS Associate Solutions Architect 2020: Identity & Access Management
https://share.percipio.com/cd/JrXPzVbEL


---------------------------------------Udemy Ranga AWS Course----------------------------------------------------------------------

Provisioning (renting ) resources and don't buy
Also called on-demand resource provisioning
rent based on the demand

Pay per use
No upfront planing required
avoid undifferentiated heavy lifted

Challenging
building cloud enabled applications

root user have both programmatic and AWS mgmt console

policy can be assigned to an individual users as well as on a group
called Managed policies (ie managed by AWS)

Regions and Zones

1) slow access from other parts of the world (High Latency)
2) what if that data centre crashes (low availability)
3) what if entire region go download

Most Services are regional services
region can be changed 

Global Services
ie IAM  can't change regions

Choose region based on
	ur users are located
	ur data is located
	regulatory and security compliance

Availability zone
isolate locations  in region
Each zone has at least two AZs
Increase availability of applications in the same Region
availability zone is physically separated from each other
AZ consists of data centres
AZs are connected through high through put network, low latency

US West (Oregon) in cosole = us-west-2 in CLI

Choosing Regions
availability of the service in that region (most services we require)
As Each region expose is regional services endpoints 
latency, cost and data residency (complaint with business contractual reqiments)


Region
ap-south-1

Availability Zone (ends regions with a alphabet)
ap-south-1a 
ap-south-1b
ap-south-1c

They provide High Availability and Low Latency

AWS SDK
interact aws services from our code

AWS as Identity provider 
its a managed service 
single point of failure, high available,

Control Plane
Data Plane

AWS Federation
centrally managed to access AWS resources
single singn on
SAML security access markup language 
cross account acccess
OPENid connect (OIDC) tokens 
AD is a common Identity provider

AWS SSPO can interact with Active Direcoity
Amazon Congnito 


account name, pwd and email address of root user can be changed 

MFA 
multi-factor authentication 
can be used to delete S3 objets

IAM Roles
delegate access eg. apps to EC2 for S3 bucket
cross-account access
	crete IAM policy to trust account
identity federation (outside aws ) using identity broker application


IAM --> Security Status -- Activate MFA
Activate MFA
Virtual MFA device
Install Google Authtication to your mobile
show the QR code
after registering the code on the device I need to 
provide the code


AWS Support offers four support plans: 
Basic, Developer, Business, and Enterprise.


----------------------------------------------EC2-----------------------------------------------------------------------------------
elastic compute cloud
ec2 instances are virtual server
billed by second
EC2 service provisioned EC2 instances/virtual servers
	creates and manage life cycle of ec2 instances,
	load balancing and auto-scaling ,
	attach storage
	manage network

Security Group
is like a Virtual Firewall having inbound/outbound rules

Instnce Types
compute CPU,GPU

t2.micro
t is instance family (t is General purpose instance)
2 is generation 
micro size
   nano<micro<small<medium<large<xlarge

1) Change Region to 
	Asia Pacific (Mumbai) ap-south-1
2) EC2 --> Create instances
3) AMI Amazon Machine Image
	Amazon Linux 2 AMI (HVM), SSD Volume Type	
	
4) Security Group
	my-EC2-security-group	
	
5) create new keypair file
and download EC2-keypair.perm at 
C:\Users\AliImran\Box Sync\P52\My-Learning\My-AWS\Architect\in28Minutes-AWS\LABS

6) Connect
EC2 Instance Connect (browser-based SSH connection)

whoami
python --version

6) Security Group 
Add

HTTP
AnyWhere

All ICMP ipv4  (used for ping public ip)
AnyWhere


7)
Open Command Prompt
cd C:\Users\AliImran\Box Sync\P52\My-Learning\My-AWS\Architect\in28Minutes-AWS\LABS
ssh -i "EC2-keypair.pem" ec2-user@ec2-13-234-77-166.ap-south-1.compute.amazonaws.com


8) 
sudo su
yum update -y
yum install httpd
systemctl start httpd
systemctl enable httpd
echo "Hello World" > /var/www/html/index.html
 

 
curl -s http://13.234.77.166/latest/dynamic/instance-identity/document > /var/www/html/index.html

1) EC2 instance metadata service 
get details about EC2 instance

URL accessible within he EC2 instance only
http://url/lates/meta-data/

curl http://ec2-13-234-77-166.ap-south-1.compute.amazonaws.com/latest/meta-data/ami-id/
curl http://13.234.77.166/latest/meta-data/hostname
curl http://13.234.77.166/latest/meta-data/instance-id
curl http://13.234.77.166/latest/meta-data/instance-type



curl -L http://13.234.77.166/latest/meta-data/

You can get
AMI Id
Storage  Devices
DNS hostname
instance id
instance type
security groups
IP addresses

2) dynamic data service

curl http://13.234.77.166/latest/dynamic
curl http://13.234.77.166/latest/dynamic/instance-identity
curl http://13.234.77.166/latest/dynamic/instance-identity/document
 

9) 

echo "EC2 Instance-2 with ($(whoami) ) on host ($(hostname)) with ip ($(hostname -i))" > /var/www/html/index.html

curl http://localhost/latest/dynamic/instance-identity/document > /var/www/html/index.html


Security Group 
defence in depth

if there is no rules , no traffic is allowed 
you can specific only allow rules only rest is not allowed by default
can assign upto 5 security groups
No restart is required for Ec2
* Security Group are stateful , if outgoing/incoming is allowed, the incoming  response for it is automatically allowed
timeout in case of security group not allowed

EC2 IP Addresses

Public address
internet addressable

Private address
internal to the corporate network

Can't have two resources with same public IP address
however, two diff corporate networks can have resources with same private IP address
All Ec2 instnces are sigged to private I{P address but public is not assiged autmatically
Stop/Start Public IP is changed but private is same 
reboot will not change public IP

------------Elastic IP-----
constant IIP
Normally  where 1 instance is required it is ok

Go to Ec2
Elastic IPs
Allocate an IP
Action Associate with instnace

Now in instance the public IP is associated with the elastic ip
when instance is stopped even elastic IP is there 

*** Elastic IPs can  be swithced to other ec2 insce within the SAME region
they need to be manually detached
*** If elastic ip is not in used or if ec2 instance associated with it is stopped  then u will be charged
better is to relase the elatic ip

----------EC2 HTTP Server Setup--------------
userdata
script used at launch of the instance

bootstraping 
install os patches or sofware when ec2 instance is launched

Create another Ec2 isntance

Add below in the userdata
#!/bin/bash
yum update -y
yum install httpd
systemctl start httpd
systemctl enable httpd
echo "EC2 Instance-2 with ($(whoami) ) on host ($(hostname)) with ip ($(hostname -i))" > /var/www/html/index.html


sudo su
yum update -y
yum install httpd
systemctl start httpd
systemctl enable httpd
echo "EC2 Instance-2 with ($(whoami) ) on host ($(hostname)) with ip ($(hostname -i))" > /var/www/html/index.html

cd C:\Users\AliImran\Box Sync\P52\My-Learning\My-AWS\Architect\in28Minutes-AWS\LABS
ssh -i "EC2-keypair.pem" ec2-user@ec2-35-154-93-55.ap-south-1.compute.amazonaws.com


http://ec2-35-154-93-55.ap-south-1.compute.amazonaws.com

----------------------------------------------Launch Template userdata-------------------------------------------------------

Use launch templates to automate instance launches, simplify permission policies, 
and enforce best practices across your organization. Save launch parameters in a template 
that can be used for on-demand launches and with managed services, including 
EC2 Auto Scaling and EC2 Fleet. Easily update your launch parameters by creating a 
new launch template version.

EC2 --> Launch templates --> Create launch template

Name
MyEC2Template

AMI
Amazon Linux 2 AMI (HVM), SSD Volume Type - ami-09a7bbd08886aafdf (64-bit x86) / ami-011abae98ef4c1bfa (64-bit Arm)

Instance Type
t2.micro

Keypair 
EC2-keypair

Security Group
my-EC2-security-group

userdata

#!/bin/bash
yum update -y
yum install -y httpd
systemctl start httpd
systemctl enable httpd
echo "EC2 Instance with ($(whoami) ) on host ($(hostname)) with ip ($(hostname -i))" > /var/www/html/index.html

Go to Instances
Launch Instance from template
Select MyEC2template

http://ec2-3-7-66-215.ap-south-1.compute.amazonaws.com/
EC2 Instance with (root ) on host (ip-172-31-14-110.ap-south-1.compute.internal) with ip (172.31.14.110)

http://ec2-13-233-53-139.ap-south-1.compute.amazonaws.com
EC2 Instance with (root ) on host (ip-172-31-13-231.ap-south-1.compute.internal) with ip (172.31.13.231) 


----------------------------------------------Customised AMI-------------------------------------------------------

for installing OS patches and software using userdata at the launch of EC2 isntance increases bootup time
also called Hardening an Image cusomised EC2 images to your ciporate securiyt standards)

Select a running instance
Action - Image -- Create Image
Name
MyCusomizedEc2AMI

Create Image
Once Image is created go to

Go to AMI
Now Launch Template (template can have multiple version)
Action - Modify template
create new version  V2

Go to Template
Create instance from template
select V2

AMI Image contains
OS and software on that instances
have root volume also attach non root volumes
can be shared with other accounts (Permissions -- add account no.)
AMI are stored in S3 and are region specific, so can be created instancees in AZs of that region
better to backup AMI in other regions for DR (Copy AMI to other regions)

Three AMI sources
1) by AWS
2) AWS market Place  Per hour billing
3) Customised AMI


AMI - Action -- DeregisterAMI
SnapShot- Action- Delete

Trouble shooting
EC2-keypair.pem is the private key
chmod 400 EC2-keypair.pem (read only to the owner)
as 0777(default , v open) not recommended
use public DNS or IP to connect 


for windows instances
1) private key
2) admin pwd
decrypt the pwd using private key and login via RDP
3) Security group allow (otherwise timeout can probably come)
SSH 22
RDP 3389

EC2 uses public key cryptography using RSA
public key is stored in EC2 instance
private key is stored by customer


For putty
create ec2_instance1_KP.ppk using puttygen and coonect to EC2 instance

If you stopped EC2 instances then still EBS volumes (harddisk) will be there and u will be charged

----------------------------------------------Good EC2 Scenarios------------------------------------------------------

Q1:  identify all instances belonging to a project, to an eviroment
SOL: tags

Q2: Change instance type
SOL: stop the instance and then change its type

Q3: don't want an EC2 instance to be automatically  terminated 
ie from Action-Instace Sate- Terminate
SOL: Change Termination Protection to Enable
also can be set during instance creation
But EC2 Terminal protection is not effective for termination from
1) Auto Scaling Groups (ASG)
2) Spot Instances
3) OS shutdown

Q4: Update EC2 instance to a new AMI with latest patches
SOL:create/relaunch a new instance with updated AMI

Q5: create EC2 instances based on on-premise VMs
SOL: yes, using import/export , you are responsible for licenses

Q6:Changing security group
SOL: easily chnage/delete securoy group to an instance
multiple security group can also be assigned

Q7: Timeout
SOL:	inbound rule for security group to check

Q8: Installing a lot of woftwares sing userdata that slowing down instance launch
how to make it faster
SOL:	cusom AMI-------------------------------------------------------

Q9: stopped EC2 instance, will I get bill
SOL:	if u have storage 

Turn On Termination Protection to protect EC2 instances for termination
as this option will disable terminate menu

-------------------------Creating EC2 in different Zones--------------------------------------------------------------------------------------------------------

No. of Availability Zones in a Region = No. of subnets 

Services - VPC

A VPC is created default in every region


----------------------------Billing in AWS---------------------------------------------------------------------------------

1) Set Billing Alerts
2) Monitor Every day for first week
3)

My Account
IAM User and Role Access to Billing Information
Activate IAM Access

Billing Alerts
My Billing Dashboard
Billing Preference

CloudWatch

1) At the moment, CloudWatch displays all billing data and alarms in US East (N. Virginia)
so switch to it

2) Select Billing

3) Create Billing alert

4) create SNS topic
CloudWatch_Alarms_Topic

monitoring service and trigger alarm
create alarm
create topic


1) Gto to Budget
2) create a new budget 
MonthlyBudget

3) Monthly
4) Budgeted amount 
$1

----------------------------------Load Balancing---------------------------------------------------------
Elastic Load Balancer (managed service, and auto scaled (highly available)
used for LB and distribute load for EC2 
these EC2 instances can be in multiple AZ within a region
ELB can be public (accessible over internet ) or private to aws network
distribute loads to the healthy instances using health check

Each layer makes use of layers beneath it

App Layer layer 7  (HTTP, SMTP)  make api calls

Transport Layer 4  TCP TLS (secure TCP), UDP(high performance over reliability)  ensure bits/bytes transferred properly and order

Network Layer Layer 3 (IP) transferring bits/bytes
 
 3 Types of ELB
 
 (1) Classic LB (Layer 4 (TCP/TLS & UDP) and layer 7 (http/https) )
not recommanded by AWS
older version of ELB


2) Application Load Balancer (Layer 7)
new generation , advance routing ,http/https

3) Network load Balancer (Layer 4)
very high performance TCP/TLS & UDP

Create a Classic Load Balancer

1) AWS Services -> EC2 - Load Balancer-> create loadbalancer
Type: Classic Load Balancer (Previous Generation)
Choose a Classic Load Balancer when you have an existing application running in the EC2-Classic network.
Name: My-Classic-LB
Security Group : My-LB-Security-Group

Health Check
Ping Path: /
Response Timeout: 5 sec
Interval: 20 sec
Unhealthy Threshold: 22) 
healthy Threshold: 220	

Add EC2 instances:

Enable Connection Draining : 300 sec



EC2 Instance 1
http://ec2-3-6-94-201.ap-south-1.compute.amazonaws.com
EC2 Instance with (root ) on host (ip-172-31-35-58.ap-south-1.compute.internal) with ip (172.31.35.58)

EC2 Instance 2
http://ec2-13-234-204-76.ap-south-1.compute.amazonaws.com
EC2 Instance with (root ) on host (ip-172-31-34-29.ap-south-1.compute.internal) with ip (172.31.34.29)

Load balancer  DNS
http://My-Classic-LB-1642051479.ap-south-1.elb.amazonaws.com

Now Delete the Load Balancer using Action

2) Application Load Balncer  layer 7
most frequent used
supports websockets http/https
supports all supporting load balancer features
scale automatically
it is a managed service
but we are responsible of scaling EC2 instances
it can also LB with container applications/ web app
also can lB using Lambdas 


EC2 - Load Balancer - Application Load Balancer (Type)


Name: My-Application-LB
Internet-facing
ip address type: 	ipv4

select all availability zones

Cross Zone LB is enabled defaulty

Security Group (to control traffic using rules) : My-ALB-Security-Group

Configure Routing
Your load balancer routes requests to the targets in this target group using the protocol and port that you specify, and performs health checks on the targets using these health check settings. 
***Note that each target group can be associated with only one load balancer

--------Target Group-------------------------------
Target Group (used to group EC2 instances/lambday/ or set of IP ) for LB to distribute load)
My-EC2-Target-Group1
type: instances

De-registration delay
How long should ELB wait before de-registering a target.
This setting ensure that the load balancer gives in-flight requests a chance to complete excution
default is 300 seconds upto an hour , also called Connection draining
eg .it will not enterain new request but wait for 300 sec and then de register the ec2 instance

slow start duration
when a target is ready 
eg. 10 sec after 10 sec it will send request to newly instance

Algorithm
Round robin 1 by 1
Lest outstanding request 

Stickiness
enable  (for session mgmt, send all request by a same user to the same instnave
imlemented using cookie
supported by ALB and CLB

-------------------------------

Security Group : My-LB-Security-Group

Register Targets
Register targets with your target group. 
If you register a target in an enabled Availability Zone, the load balancer starts routing requests to the targets as soon as the registration process completes and the target passes the initial health checks.

select all instances - Create
DNS Name
http://My-Application-LB-284230688.ap-south-1.elb.amazonaws.com


To restrict traffic of EC2 instacnes directly insested of Load Blancer

Select EC2 instance 
Edit Security Group
Inbound Rule
Add Load Balancer Security group


Add a new Listner
port 8080
response 200
fixed response

Security Group of LB
add inbound
8080

http://My-Application-LB-284230688.ap-south-1.elb.amazonaws.com:8080


-------------------------------------------------------------------------------------------------------------------
User data for Microservice


** One ALB can support multiple Microservice
Create a separate target group for each Microservice

*** Classic LB doesn't support multiple target groups


Listener Rules
configure multiple listener rules for the same listener
rules are executed in the order they are configured
default rule executed at the end


Create a new EC2 instance from Template
Select instance with Tag  InstanceForMicroserviceA
add userdata as below

#!/bin/bash
yum update -y
yum install -y httpd
systemctl start httpd
systemctl enable httpd
echo "EC2 Instance  for MicroService Awith ($(whoami) ) on host ($(hostname)) with ip ($(hostname -i))" > /var/www/html/index.html
mkdir /var/www/html/a
echo "Microservice A Called with ip ($(hostname -i))" > /var/www/html/a/test.html


create a new Target Group TargetGroup-MicroserviceA
Register new EC2 instance with that Target Group
Go to My-Application-LB - > Select exisitng HTTP Listner --> view/edit rules
Add Rule
if Path /a/*  THEN forward it to TargetGroup-MicroserviceA


http://my-application-lb-284230688.ap-south-1.elb.amazonaws.com/a/test.html
Microservice A Called with ip (172.31.10.46)

Lister Rules possibilities
1) based  on path = test.com/a to target group A`
2) based  on Host = a.abc.com  to target group A`
3) HTTP header and methods (GET/POST)
4) Query String   (/microservice?name=A)  o target group A , (/microservice?name=B)  o target group B
5) Based on IP all request from a range of IP to a TG A, other to TG B

 Each Lister have (protocol + port)

---------------------------Auto Scaling Groups`------------------------------------------------------------

scale out : increase instances
scale in:    decrease instances
 
ASG maintained a configured no. of instances
scale in and scale out automatically to adjust load
ASG can launch on-demand,spot or both
Best practice: use Launch template with ASG
ALB can adjust and distribute load to healthy instance



EC2- > AutoScaling Group
Create ASG
Auto Scaling group name: MyEC2AutoScalingGroup
	Launch Template (EC2 instance size,AMI) - create new launch template -> My-ASG-Launch-Template
	Source Template: MyEC2Template with latest version
Add all Subnets
Enable load balancing = YES
Create New Target Group = ASG-TargetGroup (with  no. EC2 instances registered)

Configure group size and scaling policies
Desired Capacity = 2
Min Capacity 		 = 1
Max Capacity 		 = 3

Policy 
TargetTrackingPolicy
Policy type:
Target tracking scaling
Execute policy when:
As required to maintain Average CPU utilization at 70
70

*** Even  if policy indicates, ASG ensure the desired capicty 

IMP use cases

1) maintained current instance levels at ll times
min=max=desired == CONSTANT
when always there is constant load expected

2) Scale Manual
change desired capacity as needed
you need complete control over scaling 

3) Scale based on schedule
programs with regular schedules
Create Schedule Action

4) Dynamic/Automatic Scaling 
On demand Scale
Unpredictable load

Types of Dynamic Scaling

1) Target Tracking
Maintain CPU utilisation at 70%
Modify current capacity based on the target value for a specific metric

2) Simple Scaling
more complex policy
+5 if CPU utilisation > 80%
-3 if CPU utilisation < 60%
waits for cool down period before triggering additional actions

3) Step Scaling
+1 if CPU utilisation between 70% and 80%
+3 if CPU utilisation between 80% and 100% 
Warm up time can be configured for each instance.

Cloud watch
monitor metrics 

CW Alarm -> Auto Scaling -> EC2 instances

CW Alarm (if CPU > 80%)
Scaling Action (+5 EC2 instances)

CloudWatch --> Alarms

Auto Scaling Scenarios

1) change instance type or size of ASG instances
OR 
roll out a new security patch (new AMI) to all ASG instances.
Sol:
launch configuration or launch template cannot be edited
create a new version and ASG use that  version
Terminate instances in small groups to make sure application is available

2)  Perform actions before instance added/remove
Create a lifecycle hook , configure clodwatch to trigger actions based on it

Auto scaling Group --> Instance Management --> Life-cycle hooks
create life-cycle hook

MyLiifeCycle

3) which instance is terminated first when scale in
based on termination policy
default is distribute instances across AZ then based on old instance


EC2 -> Auto Scaling groups -> MyEC2AutoScalingGroupASG 
Advance configurations
Termination policies


4) Prevent frequent scale up/down
Adjust cooldown perod to a high value default 300 sec

5) I want to protect newly launched instance from scale-in 
Enable instance scaling protection
if enabled then all new instaces from this ASG will not be scale in


Deleting LB and Target groups

1) Delete ALB
2) ASG Delete (will automatically delete the instances)
3) Delete Target Groups


---------Network Load Balancer---------layer 4 UDP-----------------
*** A static / Elstic IP can be assigned with NLB
can load balance EC2, ECS (container- appl, web apps using IP address)
No support for Lambdas

Name: My-Network-LB
Protocol Listener : TCP port: 80

Configure routing
TargetGroup : TargetGroupForNLB
Register Targets: Add instances

DNS
http://My-Network-LB-69ad0058bb90cba2.elb.ap-south-1.amazonaws.com

** There is no security groups at the NLB

Attributes that can be changed 
Delete Protection
Cross Zone load balancing
Access log`ie sent to S3


1)  Delete NLB : My-Network-LB
2)  Delete TargetGroup : TargetGroupForNLB
3)   Terminates all instances


Both ALB and NLB supports sticky sessions

------------------------------------Availability-------------------------------------------------------------

Are applications are available when users need them
Percentage of time
99.99% is four 9s availability

99.95% 		22 mins down time in a month
99.99%   	4.5 mins
99.9995 	26 seconds 

To achieve Availability

1) use LB
2) deploy EC2 instances to multiple AZ's
3) use cross Zone LB
4) Deploy to multiple region
5) configure EC2 and ELB health checks

ALB by default enabled Cross Zone LB
CLB and NLB it need to be enabled

----------------------Scalability ----------------------------------------

Ability to serve more growth increase proportionally with resources.
handle user growth, traffic and data size without effecting performance

1) Deploy to bigger instance vertical scale
hd,cpu,ram,i/o
there are limits to VS
it is expensive
increase EC2 instance size ie t2.micro to t2.small

2) Horizontal scaling
Deploying multiple instance of app/db
it also increase availability

Additional infrastructure needed

Distribute instances

1) Distribute EC2 in a single AZ in same Region
2) Distribute EC2 in a multiple AZ in same Region
3) Distribute EC2 in a multiple AZ in multiple Region

Auto Scale Group

Distribute load : ELB , Route53(for multiple regions)
ELB is managed so no need to think about its scaling

EC2 instance Families

m ( m4,m5,m6 General purpose)
web servers and code repos

t (t2,t3,t3a) Brustable performance (accumulated CPU  credits)
workload with spikes, dev env , small db
unlimted mode (spike beyond CPU credit at additional costs) off for t2 but on for t3 by default


c (c4,c5,c5n ) Compute optimised High performance
batch processing,High performance compute

r (r4,r5,r5a,r5n) Memory Ram  optimized
in mem db, real time analytics

i (i3,de)  storage optimised
NoSQL DB and data warehousing

g (g3,g4) GPU optimised 
video compressio, grapic processing


f  (f1) FPGA instances 
massive parallel processing power as geonomic, data analytics

inf (inf1) Machine learning ASIC instances
image,speech,nlp recognition 

Tenancy Options
shared vs dedicated

default is shared

E2 dedicated instances
virtualised instances on hardware dedicated to a customer
no visibility into the hardware of underlying host
billing per instance

E2 dedicated hosts
physical servers dedicated to one customer
regular needs, high security
billing per host
server bound software licences like windows server, sql server

------------------------Placement Groups-------------------
low latency net=work communication (instances on a same RAC), Cluster
high availability (instance on diff RAC) , Spread
Partition (multpile partitions with)

READ AAGAIN

Elastic Network Infterface

like a virtual nework inerface card
primary ENI is by default and can't be detached

ENI lives in one subnet and thus in a single AZ
one primary private IPV4 and one or more secondary PV4
one Elastic IPv4
one public IP IPV4
one or more security groupa mac addeess
ipv6

No. of EC2 isntances = No. of ENI

EC2 Partition Placement Group
In large distributed and replicated workloads (HDFS, HBase, and Cassandra), EC2 instances need to be divided into multiple groups.

------------------------------------------------------------------------------------------------
Monitoring EC2 instances
using 
CloudWatch
basic monitoring every 5 mins Free
CPU,Disk,Network are tracked by cloudwatch

CW doesn't have access to OS system metric like memory consumption
1) install cloudwatch agent on ec2
2) use cloudwatch collected plug-in

-----------EC2 Pricing Model ------------------------------------------------------

1) On demand
Request when u want it
Flexible and most expensive
ideal for spiky traffic 
batch program having unpredictable runtime

2) Spot
Quote the max 
Cheapest upto 90% off But not guarantees
not for critical apps, non time-critcal workloads
old-model = bid a price, biggest bidder wins
new model: quote your max price,based on long term trends
can a terminated with a 2 min notice
best practice: stop or hibernate instance on receiving interruption notice as for terminate a new instance will be allocated

Spot Block
request spot instances for a specific duration (1,2 hrs up to 6)

Sopt Fleet
Request spot instances across multiple instance types(micro,small,large)
to get better change to have a spot instance


3) Reserve ahead of time
up to 75% 1-3 years

4) Saving plans 
commit spending $ per hour on (EC2 or Fargate or Labmda)
up to 66% 
No restriction


---------------Launch A spot instance ---------------------------------
EC2  --> Choose instance type --> configure instance

Purchasing option --> Request Spot instances

Maximum price: 	
The maximum price you are willing to pay per instance hour. 
Your instance runs when your maximum price is greater than the Spot Price

Persistence request: 
Ensures that your request will be submitted every time your Spot Instance is terminated. 


EC2 > Spot Requests > Request Spot Instances 

for spot block use below option
Defined duration workloads Launch instances into a Spot block for 1 to 6 hours.

for spot fleet use below option
Load balancing workloads Launch instances of the same size, in any Availability Zone. Good for running web services.

Fleet request settings


EC2 Linux Spot instances - Pricing

Zero charge if terminated or stopped by Amazon EC2 in the first instance hour (60 mins)
otherwise u will be changed by seconds

terminated/stopped after 50 mins by Amazon = you pay 0
terminated/stopped after 50 mins by you = you pay for 50 mins
terminated/stopped after 70 mins by you/Amazon = you bay for 70 mins


when interrupted, spot instances can be terminated (default),stopped and hibernated

Completely close a spot request  in the following order for best practice

1) cancel spot request  (only this can't terminate active spot instaces)
2) terminate all spot instances

---------------------EC2 Reserved instances ------------------------------
Reserve E2 instances ahead of time

when u have constant worloads that run all the time
3 types
1) STANDARD

In a region, i reserve EC2 instance with a specific platform,instance type for a term of 1 or 3 years.
can switch to other instance sizes within the same instance family ie t2.micro to t2.small,large
AZ can also be changed
Instance family , os or tenancies (shared/dedicated) CAN'T be changed
up to 75% off

2) CONVERTIBLE (with  some flexibility)

In a region, i reserve EC2 instance for a term of 1 or 3 years.
can switch to other instance sizes within the same instance family ie t2.micro to t2.small,large
AZ can also be changed
Instance family , os or tenancies (shared/dedicated) CAN be changed
up to 54%

3) SCHEDULED
in a region, i reserve an EC2 instance par-time for a year, x hour every month/week/day at a specif time
restriction: available in few instance types C2,R3,C4,M4 in few regions 
use case: bill are generated on the first day of month
5 to 10%


You can also sell reserved on AWS reserved instance marketplace if you don't want to use your reservation.

PAYMENT
1) 0 upfront, pay monthly instalment
2) partial upfront ,
3) All upfront, 0 instalment 
all upfront < partial upfront < No upfront (a diff upto 5 %)

Saving Plans
Constant workloads that ru


https://www.ec2instances.info/

----------------Elastic Load Balancer Advance------------------------------------------------------------------

for Https we need to install ssl or tls certificates on the server
in AWS SSL certificates can be managed using AWS Certificate Manager

ELB requires X.509 certificates (ssl/tsl)

Application/Classic LB SSL TLS Termination
Client -> ELB 		https
ELB -> EC2        http

Network LB TLS Termination
Client -> NLB TLS
NLB -> EC2    TCP

---------------------------------------------Server Name Indication----------------------------------
Each listener in ALB can be associated with multiple SSL certificates (one for each website)
and SNI ServerName Indication is automatically enabled
its extension to TLS

--Monitoring ELB logs and Headers -------------------------------

ELB --> Action
Edit LB attributes
Access logs 

NLB allows EC2 intances to see client details like IP but not ALB in request header

X-Forwarded-For      Client IP
X-Forwarded-Proto   Originating protocol
X-Forwarded-Port     Originating port


IMP ALB VS NLB VS CLB read again
---------------------------------------------------------------------------------------------------
IMp LB scenarios 

1)maintain sticky session
enable stickiness on ELB (cookie name normally: AWSELB)

2)distribute load only yo healthy instances
configure health

3) configure load among 2 AZ in the same region
Enable cross zone load balancing

4) in-flight request to unhealthy instances to given opportunity to complete
enable connection draining 

5) warm up time to EC2 instances before getting load
configure health check grace period

6) protect EBL from web attacks, sql injection, cross-site-scripting
integrate ELB with WAF web application Firewall

7) Protect web app from DDoS attacks 
by default ELB provide it

-----------Architectural Considerations ----------------------------------------
 Security
 use security groups to restrict traffic
 EC2 instances in private VPC (not accessible out side AWS network)
 user dedicated hosts when u have regulatory needs
 
 Performance
 right instance family
 appropriate placement groups
 prefer custom AMI to install softwares using user data 
 choose right ELB for you use case

Cost
optimal no. and type of EC2 instance family
use right mix of 
saving , reserved , on demand , spot instances

Resiliency (how quickly u recover)
health checks
cloudwatch for monitoring
DR , ami coppied to multiple regions


warm attach = attaching ENI with EC2 instnace is stopped
spoot is the cheapest
on deman is the costly


------------------------------------------------------------------------------------

IAAS (Infrastructure as a Service)
use only INFRASTRUCTURE from cloud provider
also called Lift and Shift
eg. EC2 to deploy your applications or db

You are responsible for 
1) app code and runtime
2) configure LB
3) auto scaling
4) OS upgrade and patches
5) Availability

PAAS (platform as a service)
use a PLATFORM provided by cloud
eg EBS Elastic Beanstalk

alternative to PAAS
CAAS (container as service)container instead of applications
ECS Elastic Container Service

FAAS Function as a Service
serverless
not worrying about servers`
AWS Fargate Serverless compute for container


Cloud provider provides
1) OS
2) App runtime
3) auto scale-in, lb

You are responsible
1) Application 
2) configurations

-----Elastic Beanstalk --------------------------------------------
simplest way to deploy and scale web apps in AWS

supports java,.net,node.js,php,python,go and docker apps
Its free but chnarges for resource being consumed

automatic LB
Auto scalaing
platform update health monitoring


***** Elastic Beanstalk is not EBS (Elastic bean store)


An application can have multiple environments to be created
An  application can have multiple versions (stored in S3)

Elastic Beanstalk

Name: My-First-ElasticBeanstalk-App
Platform: Python
Application Code: Sample application

It will create an 

Environment Tier

Web Server environment (for web apps)
Worker environment (for batch )

You can retain FULL control over ASWS resources created
It is ideal for simple web applications
Not ideal for Microservice architectures
access server logs without logging into the server
logs can be stored in S3 or in ClodwWatch
can configure SNS based on health
can apply patches and platform updates


Environment : MyFirstElasticbeanstalkApp-env
Application: My-First-ElasticBeanstalk-App
EC2 instance created : i-09198c2b1b80e3fcd
a LB is created (can make changes in ELB) : awseb-AWSEB-ZV73FYBTVZ9S

URL: http://myfirstelasticbeanstalkapp-env.eba-jsbz4ebm.ap-south-1.elasticbeanstalk.com/

To Delete
1) Delete the application
it will delete EC2 and ELB 



------------------------Containers Solutions------------------------------------------------------------
Docker is cloud Neutral

Container Orchestration provides
Auto scaling (scale container based on demand)
Service Discovery (helps Microservice find others)
Self healing (replace failing instances using health check)
Zero Downtime deployments release new versions without downtime

Container Orchestration Options 

1) Cloud Neutral
	Kubernetes 
	AWS Service = AWS Elastic Kubernetes Service (EKS)
	EKS doesn't have a free tier

2) AWS Specific
	AWS Elastic Container Service ECS
	AWS Fargate (No free tier): Serverless version of AWS ECS



AWS Elastic Container Service ECS (A regional Service)
Need to create a cluster of EC2 instance managed by ECS for Microservice
build using container images

For AWS Fargate 
No need to manage EC2 instances 

Use case 
Microservice
batch processing on ECS using AWS Batch

ECS and Fargate points to Elastic Container Service

Don't Create 

Container definition
Amazon ECS makes it easy to deploy, manage, and scale Docker containers running applications, services, and batch processes. Amazon ECS places containers across your cluster based on your resource needs and is integrated with familiar features like Elastic Load Balancing, EC2 security groups, EBS volumes and IAM roles

Sample Application
Container Name: sample-container-Image
Image: httpd:2.4


A task definition is a blueprint for your application, and describes one or more containers through attributes. Some attributes are configured at the task level but the majority of attributes are configured per container.

Define your service
A service allows you to run and maintain a specified number (the "desired count") of simultaneous instances of a task definition in an ECS cluster.

Service Name: sample-app-service
Number of desired tasks: 2

Application Load Balancer

Configure your cluster
The infrastructure in a Fargate cluster is fully managed by AWS. Your containers run without you managing and configuring individual Amazon EC2 instances.


For Fargate
No EC2 instance created
LB is created
Target Group is created

A Task is a definition of a container
Tasks role can also be attached (RDS)
Task execution IAM role (permission to pull container images)


A existing task deification can't be changed
A tasks has IP and container details

A Service  allows to run and maintain  a specific  no. of tasks  (desired count)
it brought up new tasks in case of failure

ECS Cluster
Groping of one or more container instances (EC2 instances) when   u run your taksk

EC2 instances in a cluster running a container agent for communication

ALB features
Dynamic host port mapping  (multiple task from the same service allowed per EC2 container)
path based routing multiple services can use same listener port on same ALB and be routed based the path


Elastic Beanstalk can run container using Docker as platform but can't create clusters

Amazon EKS 
recommended if you are already using Kubernetes and want to move workload dto AWS

ECR 
Elastic Container Registry
for deploy container images



------------------
To have more access to EC2 instances for ECS
ECS --> Cluster-> Create Cluster -- Select cluster template

EC2 Linux + Networking
Resources to be created:
Cluster
VPC
Subnets
Auto Scaling group with Linux AMI



---------------------------ServerLess-----------------------------------------
You don't worry about  infrastructure
flexible scaling and automated high availability
Pay for use 
	No of requests
	duration of requests
	memory consume

Lambda called/trigger from 
Amazon API Gateway
AWS Cognito
DynamoDB (event)  chnage in DB
CloudFront (lambda@Edge)
AWS Step Functions
Kinesis (event)
S3 Simple Storage Service , new object
SQS (event) new msg in Queue
SNS Simple notification service and so on


name: MyNodeJsLambda1
runtime : Nodejs
create Event: FirstEvent
Test

create environment variable 
EnvironmentName = Development

add beow code in index.js
exports.handler = async (event) => {
    // TODO implement
    const response = {
        statusCode: 200,
        body: JSON.stringify('Hello Ali from ' + process.env.EnvironmentName),
    };
    return response;
};


AWS X-Ray (tracing)
to trace request
request granted for role
Enable it 

Matrix using AWS cloudwatch (monitoring and log)


basic setting
MEMORY = the more memory the more CPU
Price proportional to Memory 

TimeOut  of lambda function  = 3 seconds is default 
max timeout = 15 mins

stateless - store data to S3 or DynamoDB
500 MB of non persistent distk /tmp
Allocate memory in 64MB increments from 128MB to 3 GB


-----------------------------API Gateway-----------------------------------------------------

Rest API Challenges

Management of Rest API is not easy
authentication and authorization
Rate limits (quotas)
Can  Run Multiple Versions of API
Monitor API callsCache API requests
API keys for third party developers 

Full Managed Service
publish , maintain , monitoring and securing APIs
Integrate with Lambda, EC2, ECS,  or any publicly addressable web servic
supports HTTP(s) and web sockets
It is serverless 

HTTP API is new version of REST API


REST API -->  New API
Name: MyHelloWolrdAPI
EndPoint Type: Regional

Regions = 20 
Edge Locations = where regions are not available CloudFront distribute content 200+

Create API

Action-> Create Method -> Get

Integration Type:
Lambda Function: MyNodeJsLambda1
Save
Test

Authorizers
Name:
Type  Congnito and Lambda authorizer (JWT token or SAML)

API keys
To identifying API clients  based on key used for usagePlan

Throttling
wBrust: 800

Quota: 
total no of request in a month

Client Certificates:
to ensure HTTP requests to your back-end services are originating from API Gate
to verify requester's authenticity


Action -> Deploy API 
Deployment Stage:  New Stage
Stage Name:Dev

URL: 	https://ruvdffuyq5.execute-api.ap-south-1.amazonaws.com/Dev

Canary Deployment
sent a % request to a no. of users

-------------------------- Virtual  Private Cloud ----------------READ AGAIN IN DETAIL-----------------------------------------

Creating own private network in the cloud
your own isolated network (traffic not visible to other AWS vps)
you can control all the traffic coming in and out a VPC
Best practice all resources from ur VPC

To separate public resource (accessible through internet) with private resources 

create public subnet for public resources and like private
public resources can talk to private 

Each VPC is associated with a Region
each subnet is created in AZ

eg VPC = us-east-1
      Subnets = us-east-1a,us-east-1b

VPC -> 
default VPC always created without a name
default subnets also created 

No. of subnets = No. of AZ

IPv4  32 bit  (allows 4.3 billion address) most popular
127.255.255.255

IPv6  128 bit alphanumeric
  
182.82.143.132/32 is a single ip 
0.0.0.0/0 all

10.88.135.144/30
means
2 rasie power (32 -30) = 4 

Each VPChttp://localhost:7080/mep/services/pcs.ctos.lock.unlock.invoice.ext is associated with a CIDR block
CIDR b;lock can be from /16 (65536 ) to /28 (16) IP addresses

VPC with CIDR block 69.208.0.0/24  69.208.0.0 to 69.208.0.255 

There can't be an overlap of a VPC CIDR block with another connected network

All addresses inside a VPC CIDR range are private addresses
Sbunet resides in VPC

CIDR block of a subnet must be a subset or the same as CIDR block for VPC

Can VPC spread over two regions ?
No

Multiple VPC in same region 
Yes  (default vpc and custom vpc)

Communication b/w tow resources in a VPC is visible outside VPC
No

Can we allow External access to your resources in a VPC
Yes (using Internet Gateway)

Can a subnet spread over two regions ?
No, since subnet is part of VPC and VPC is associated with a single Region

Can a subnet spread over two Availability Zones
No, A subnet is specific to a particular AZ

Can I have 2 subnets in one AZ
Yes , 

Can I have subnet in AZ ap-south-1a if its VPC is in region us-east-1
No, subnet should be on AZ belonging to the VPC's region


VPC Main route table

1) each VPC when created has a main route table by default (enable communication b/w resources in all subnets in a VPC)
2)default roye rule can't be deleted/edited

3)Each subnet can have its route table or share its route tale with VPC
4) Multiple subnets can share a route table
5) A subnet can be associated with one route table ONLY

Security Groups

1) a default SG is created when we created a VPC
allows all outbound traffic
allows communication b/r resources assigned with default security grop
Denies all other in bound (other than default SG)
can be edited but not deleted
EC2 instance by default are assigned  the default security group of the VPC
Security Group can have many to many relationship with Resources (in same VPC)

New security  group
by default No in bound rule
allows all outbound traffic

ssh 22
rdp 8889
http 80
https 443
postgreSQL 5432
Oracle 1521
MySQL/Aurora 3306
MySQL 1433


1) Can source/destination of a SG be another SG
Yes


2) A new SG is created and assigned to a DB and Ec2
they can't talk as SG doesn't allow inbound traffic by default

3) A default SG is created and assigned to a DB and Ec2
they CAN talk as SG has a rule allowing traffic b/w resources with same SG

NACL Network Access Control Listener
SG control traffic to specific resources like ec2 ina subnet
NACL control traffic stopping even entering the subnet
stateless firewall at subnet level
Each subnet must be associated with NACL
default NACL allows all inbound and outbound traffic
custom crated NACL denies all inbound and outbound traffic by default
rules have priory no. (lower has high value)

Security Groups
only allow rules
stateful, return traffic is automatically allowed
traffic allowed if there is matching rule

NACL 
allow rules and deny rules
stateless, explicitly allow return traffic
rules are priority , matching rule with highest priority


Traffic from outside subnet then NACL will intercept
Traffic within Subnet SG will intercept

Imp Scenario

EC2 instance can't be accessed from internet
1) does EC2 have public IP address or an Elastic IP assigned
2) Check NACL, is inbound and outbound traffic allowed from your IP to the port
3) Check route table for the subnet, is there a route to internet gateway
if it a private subnet then can't
4) Check Security group, all u allowing inbound traffic from ur IP to the port

AWS Service to allow instances in private subnet to connect to internet to download patches
is using NAT Gateway

A VPC cannot have multiple Internet Gateways

-----------------------------------------------S3 Fundamentals--------------------------------------------------------------------------------------
Simple Storage Service
inexpensive 
store large objects with key value approach
also called Object StorageProvides REST API 
unlimited storage
objects are replicated in a single region across  multiple AZ


Buckets are fundamental container in S3

S3 -> Create Bucket
bucket name should be using  across AWS accounts
no space and special or upper case char in Name and become part of object URL
enabling object lock automatically enables Bucket versioning 
S3 is a global service 
however a bucket is created in a region
every object is asccess using key value pair
max  size of an object in  a bucket is 5 TB
No hireachy of buckets,sub buckets or foldes


Name:
my-s3-bucket-1

Upload a folder

Storage Class
Standard


2030
   10
      course1.jpg
	  course2.jpg

key 2030/10/course1.jpg    value is image 1
key 2030/10/course2.jpg    value is image 2

PATH
http://bucketName/key
s3://my-s3-bucket-1/2030/10/course2.jpg

OBJECT URL
https://my-s3-bucket-1.s3-eu-west-1.amazonaws.com/2030/10/course2.png


Bucket Level Property

Versioning 
Versioning is at Bucket level

my-s3-bucket-1 -> Properties -> Versioning
Enable Versioning


All old objects will have a version of null
We can't turn off the versioning once set

WARNING! BILLING ALERT! Do NOT Enable Access Logging For Your Buckets
Enable Logging 
Target Bucket my-s3-bucket-1
prefix logs

Permission
S3 log delivery group

Creating a Public website with S3

Static Web site hosting
Index document: index.html
on root of the bucket 
1) select all file 
C:\Users\AliImran\Box Sync\P52\My-Learning\My-AWS\Architect\in28Minutes-AWS\course-presentation-and-downloads\s3
Permissions

2) Edit  Block public access

3) Bucket policy ()resource based policy , give cross account access)

Documentation https://docs.aws.amazon.com/AmazonS3/latest/dev/using-iam-policies.html
Granting Read-Only Permission to an Anonymous User

copy readonly access and put ur bucket name

{
  "Version":"2012-10-17",
  "Statement":[
    {
      "Sid":"PublicRead",
      "Effect":"Allow",
      "Principal": "*",
      "Action":["s3:GetObject","s3:GetObjectVersion"],
      "Resource":["arn:aws:s3:::my-s3-bucket-1/*"]
    }
  ]
}


https://my-s3-bucket-1.s3-eu-west-1.amazonaws.com/index.html

Object level Logging and Encryption
using cloud trail


Encryption
None 
AES-256 (server side encryption with Amazon S3 Managed keys SSE-S3)
AWS-KMS (server side encryption with Amazon KMS Managed keys SSE-KMS))

Object Lock
prevents object from being deleted
Enable only at the time of creation bucket in advance setting, also versioning enabled
after that we can't delete objects from bucket eg. for regulatory constrains

S3 Tags
used for automation ,polices cost tracking
key values

Transfer acceleration
to improve speed of data transfer 
not free

Requester Pays
In general, bucket owners pay for all Amazon S3 storage and data transfer costs associated with their buckets. 
With Requester Pays buckets
- The requester instead of the bucket owner pays the cost of the request and the data download from the bucket
The bucket owner always pays the cost of storing data.
when this is enabled
anonymous access to the bucket is disabled

Events
Notifications to Lambda fns

Event Sources
new obj created,removal
RRS object lost events
replication across events

Events Destination
SNS Topic
SQS queue
Lambda Fn


Versioning CANNOT be configured at an individual object level?
S3 Life Cycle Configuration allows you to move files between different S3 Storage Classes?    
ACLs are primarily used to grant permissions to the public and other AWS accounts


Create a lambda function
S3NotificationLambda

Create event in S3 Bucket  in my-s3-bucket-1
S3NotificationEvent
PUT, POST
Send to Lambda Function
lambda S3NotificationLambda

Now add a file to the bucket and 

Select lamda Function 
View logs in CloudWatch

S3 Prefix
search for keys starting with a certain prefix
URL?prefix=2030/10
supported by rest api,aws cli,aws sdk, conle
Used in IAM and Bucket polices to restrict access to a specific files or group of files

http://s3.amazonaws.com/my-s3-bucket-1?prefix=2030/10
getting error

http://s3.amazonaws.com/my-s3-bucket-1/index.html


Now S3-> Permissions --> Access Control Listener--> Public Access --> List Object 

Prefix is the substr at the start of the key

it is accessible using below url and displayed the XML
http://my-s3-bucket-1.s3.amazonaws.com/
or 

http://my-s3-bucket-1.s3.amazonaws.com/?prefix=2030/10

CROS
cross origin content sharing

Block Public Access is at Highest level than access control and bucket policy
Bucket level is a policy at bucket level

Access Control List is at bucket and object levels

Use Object ACL when bucket owner is not the object owner
need diff permissions for different objects in the same bucket

IMP
Bucket / Object ACL don't have conditions but polices can have

ACL are primarily used to grant permissions to public or other user accounts


--------------------------S3 Storage Classes-----------------------
wide variety of data
huge variations in access pattern
S3 storage classes help to optimise your cost  while meeting access time needs
durability 11 9's


Standard
Frequently access data 
data replicated in at least 3 AZs
Encryption = Optional
per GB cost = 0.025

Standard-1A
long lived, infrequently access data  (eg backups for DR)
data replicated in at least 3 AZs
Encryption = Optional
per GB cost = 0.018

One Zone-1A
Non Critical data
long lived, infrequently access data  (data that can be easily generated again)
data replicated in only 1 AZs
Encryption = Optional
per GB cost = 0.0144

Intelligent-Tiering
Long lived data with changing or unknow access 
AWS choose storage classes standard or standard1A
Encryption = Optional

Glacier
Archive data with retrieval times ranging from minutes to hour
Encryption = Mandatory
0.005

Glacier Deep Archive
Archive data with that  rarely retrieval times ranging from hours  to days
Encryption = Mandatory
0.002

You can switch storage class of an object in a  bucket or during file upload

Life Cycle Management

how can you save costs and move files automatically between storage classes 
sol: S3 Life-cycle Configurations

transition actions
one storage class to another

expiration actions
delete obj eg after one month

Life Cycle Rules is NOT FREE
S3 Cross-Region Replication is NOT FREE

S3 Replication 
Can  be in same region and multiple region
Could be cross account
Access to destination is provided using IAM policy
Versioning should be enabled on BOTH source and destination
Only new objects are replicated 

Management -> Replication -> Add Rule

Object Level Configurations
select OBJECT
Properties
Can override  Storage Class, Encryption,metadata, tags
Object lock is cannot be enabled at Object level 
Permissions can be changed Object ACls 

S3 Consistency Model
S3 is distributed ans it maintains multiple copies of your data in a regions to ensure durability 

READ AFTER WRITE FOR PUTS of new object
means when u create a new objects, it is immediately available

EVENTUAL CONSITENCY for Overwrite PUTS and DELETES
means no guarantee, you might get a previous version of data immediately after an object is updated 


S3 Pre signed URL
Grant time limited permission (few hours to 7 days) to download objects
Avoid web site scraping and unintended access
using AWS SDK API
input 
security credentials, bucket name,object key , http method , expiration date time 
output 
pre signed url

S3 Access Points
simplify bucket policy configuration

access specific vpc to a specific bucket

create application specific access points with an application specifi policy

for App1 we can set different kind of action and for App2 diff action on the same bucket

Bucket -- Access points
one for each application to access

S3 Cost Factors
Cost of Retrieval Charge Per GB
Monthly Tiering (only for Intelligent Tiering)
Data Transfer Fee 

Free
Data  Transfer into S3 
Data  Transfer from  S3 to CloudFront
Data  Transfer from  S3 to Services in the same region (EC2, lambda Fn in same region)

S3 Security Scenarios

Prevent Object from being deleted or overwritten
Use S3 Object Lock

Protect against accidental deletion
Use  Versioning

Avoid Content Scraping
Pre-Signed URL also called Query String Authentication

Enable Cross Domain request to S3 hosted web site
Use CORS

S3 Cost Scenarios

1) Reduce cost 
use proper storage classes  and configure life-cycle management

2)  analyse storage access patterns and decide  right SC
use Intelligent Tiering 
use storage class analysis report

3) Move data automatically b/w SC
use lifecycle rules

4) Remove objects from bucket after a specified time period
use life cycle rules and configure expiration policy

S3 Performance Scenarios

S3 is serverless
recommended for large objects

1) Improve S3 bucket performance
Use S3 prefix
supports up to 3500 request per second to add data
supports up to 55500 request per second to retrieve data

Transfer acceleration (fast/secure file transfer to/from bucket )

2) Upload large objects to S3
use multipart upload API
 quick recovery from a network issue
 pause and resume object upload
 recommend for files > 10MB and Must for files > 4GB
 
 3 )Get some part of the object
 
 Use Byte-Range_fetches
 
 4) EC2 (Region A) accessing S3 bucket(region B)
 Not recommend , reduce network latency and data transfer cost


5) How make user pay for S3 storage
Requester pays

6) Create inventory of S3 objects 
use S3 inventory report

7) Need S3 bucket access logs
enable S3 Server Access logs (default:off)


8) change object metadata or tags or acl or invoke lambda functions for billions of objects in S3
Generate S3 inventory report and Perform S3 Batch operations using it

9)  Need S3 Object bucket Access logs
Enable S3 Server Access Logs


S3 Glacier is a separate Service 
Amazon S3 Glacier is an extremely low-cost storage service that provides secure, durable, 
and flexible storage for data backup and archival
As  a replacement for magnetic tapes
High durability 11 9's
High security as encryption is must
Cannot upload objects to glacier using management console but using 
REST API, AWS CLI, AWS SDK


S3
object are stored in buckets
object keys are user defined
allow uploading new content to the objects ie object can be modified
object size can be up to 5TB
operations at bucket and object level supported
Encryption is optional
WORM write once read many times = Enable object lock policy
can immediately  down data , synchronous 

S3 Glacier  (is a regional service)
archives are stored in Vaults
object keys are system defined
can't be updated , best for regularity compliance
object size can be up to 40TB
only vault level operations supported
Encryption is mandatory
WORM write once read many times = Enable Vault lock policyAsynchronous 2 steps 
1) initiate a archive retrieve
2) down the archive

Reduce Cost
can reduce cost by optionally specify a range or portion of archive
Requesting longer access time

Expedited 		1-5 mins  (make sure you have provisioned capacity available)
Standard   		3-5 hours
Bulk retrieval 	5-12 hours


Console --> S3 Glacier --> Create Vault
my-vault
Enable Notification
SNS topic 

---------------------------------------------IAM -------------------------------------------------------------------------------
Authentication
Authorization
Federated user (externally authenticate users) ie fb, google accounts
very granular control
to perform a single action, on a specific aws resource, from a specify ip , during a specific time window

New group 
Name: Operations
Policy : EC2FullAccess

New User
aws-opsuser-1
Add Group : Operations

Access Key ID and Secret access key is use for CLI

Add permission directly to the user
Attach existing policy directly
AmazonS3FullAccess


Policies
Create Policy
Import Managed Policy
change in JSON   
No aws icon in the custom managed policy

IAM Inline Policies
Create Inline policy
Policy Generator
Add Statement
Apply Policy



cmd
aws configure  (store aws credentials on the ec2 instance which is not a good practice)

aws s3 ls
aws s3 ls bucketName

---Roles---
Talks to S3 from EC2 instance
Create Role
Select EC2
Crete policy
S3RaeadOnly Access
Name; EC2S3AccessRole


Select EC2 instance
Action --> Attach IAM Role
EC2S3AccessRole
Apply

ls ~/.aws  (here keys were stored)
rm ~/.aws/config
rm ~/.aws/credentials

Now
aws configure
don't give access key and secret key

IAM users
users created in AWS account

IAM groups 
Collection of IAM users

Roles 
temporary identities
doest have credentials attached
adv, expire after a set period of time

Policies define permissions (JSON document) 
Permission can be assigned to users, roles , groups
Effect :can have both allow and deny
Resource: 
Action: 
Condition:

* Create a Role assign it permission with talk to S3 Bucket and assign it to EC2 instance

Instance Profile automatically created in background using Management console
Its a container (a box) for an IAM role and used to pass role info to the EC2 instance
Instance profile is a simple container for IAM roles

IAM Scenarios

1) A user in one AWS account wants to access a resource in another AWS account (cross account Access)
ie Dev AWS account , PROD AWS account
Sol: IAM Role


Corporate Directory Federation
1)user authenticated wit a corporate directory,  send token to IAM and get permissions if SAML 2.o complaint direcpty
2)for Microsoft AD then ou can use AWS directory Service
3) set up a custom proxy server to translate user identities from enterprise to IAM roles


IAM - Web Identity Federation

1) authenticate users using we identities 
eg open id (Facebook,google)

2) Amazon Congnito supports login with FB, google or other open id compatible identity providers

3) configure role to use Web Identity as trusted entity
authentication token exchanged using STSAssumeRoleWithWebIdentity API

1) Identity Based policies
attached to IAM User, Group or Role
managed and Inline
Focus: what resource , what action ?
eg. 

User Can list S3 bucket named Bucket1

user access resource directly from is aws account user can switch role 
All services supported

2) Resource Based Policy
attached to resource, S3 buckets, SQS, and AWS KMS keys
Inline Only
Focus: Who (which account, it is public), what action ?
Account A can read and modify
Public can read
cross-account access: user access resource directly from is aws account
subset of  services supported

IAM Scenarios

1) How to rotate access keys without causing problem
create a  new access key
use new access key in all apps
disable original access key
test and verify
delete original key

2) Multiple Permission resolved
By Default Deny (no explicit allow/deny)
if explicitly deny and no explicit  allow then deny
if explicitly allow and no explicit  deny then allow

IMP
IAM users identities exits until thy are explicit ly deleted (no expiration)
IAM allow you to create a password policy
Account Setting --> Set pwd policy
An IAM role can be added to a already running EC2 instance, immediate effective
An IAM role is not associated with IAM user
An IAM role is not associated with long term credentials

when a user , resource or an application Assume a Role, it is provided with temporary credentails

------------------------------Encryption--------------------------------------------
Data States

Data a rest 
in device or backup , in DB

Data in Motion/Transit
being transferred across a network
eg web page content
on premise to cloud
application in VPC talking to DB

In and Out of AWS
With in AWS

Data in Use
active data processed in a  non persistent state
eg. data in RAM

First law of security : Defence in Depth
encrypt all data 
also encrypt data in transit b/w apps and db

Symmetric Key Encryptions
use same key for encryption/decryption

Asymmetric Key Encryption (Public key cryptography)
public key
private key

Encrypt data with public key and decrypt using private key

Most Famous Algo is RSA

KMS 
Key Management Service (a multi tenant service)
manage cryptographic keys both symmetric and asymmetric
define key usage permission including cross account access
track key usage in AWS Cloud Trail (regulations and compliance)
Integrate will all aws services that need data encryption

Automatically rotate master key once in a year
Schedule key deletion
mandatory min wait period 7 days (max 30 days) 
not directly delete the key (either disable it or schedule key for deletion)

Its a managed service 

Console --> KMS --. Create Key
Customer Managed keys
Symmetric
Name: MyMasterSymmetricKey
Key Administrator
Choose the IAM users and roles who can administer this key through the KMS A

Define key usage permissions
Select the IAM users and roles that can use the CMK in cryptographic operations

Console ->S3 Bucket - Crate Bucket -Properties -
Default Encryption
AWS-KMS
MyMasterSymmetricKey

Server Side Encryption with KMS

1) create a customer master key and map to a AWS service (S3)
2) Client upload file to S3
3) S3 ask KMS to provide data keys
4) KMS use Customer master key and generate a new plain data key and a encrypted data key
5) S3 received these keys data and encrypted data key
6) S3 encrypt the data/object using data key with default Encryption Algo
7) S3 stores encrypted data with the encrypted data key in S3
8) S3 delete data key

CMK never leaves  KMS
encryption of data key - KMS using CMK
Encryption of Data - S3 using data key


Decryption

1) S3 send encrypted data key (stored with the encypted data) to KMS
2) KMS decrypt the data key using SMK and send back plain text data key to S3
3) S3 use the data key to decrypt the data

This is called Envelop Encryption 
so KMS use Envelop Encryption 

KMS encrypts small piece of data (data keys < 4KB)
actual encryption is done by the service

AWS service needs IAM permissions to use the CMK

imp:
You can associate a key/ map called encryption context with any cryptographic operation
if encryption key context is different , decryption failed

AWS Cloud HSM

Cloud Hardware Security Module
managed HA and AS
FIPS 140-2 level complaint
it is single tenant
AWS can't access your encryption master keys in CloudHSM
use two or more HSMs in separate AZ in production clusters
 AWS KMS can use CloudHSM cluster as "custom key store" to store the keys
 use cloudwatch for monitored and cloud trail for tracking key usage
  web server offload SSL processing
  as certificate authority
  digital rights mgmt
  TDE for oracle db
 if You want a dedicated hardware security module with cloud
  
 All service integrate with KMS and KMS use CloudHSM 

CloudHSM --> Create Cluster
VPC
AZs

KMS -Custom key store and connect it with CloudHSM cluster
 
 All AWS services provides HTTPS endpoints
 Encryption is optional with S3 but highly recommend in flight and at rest
 
 Server Side Encryption
 
 SSE-S3
 S3 manages its own keys
 keys rotaed every nonth
 request hearder
 
 SSE-KMS
 Customer managed keys in KMS
 
 SSE-C
 customer sends the key in every request 
 S3 performs encryption/decryption without storing the key
 Https is must
 
Client Side Encryption
Customer send encryption data to AWS service
Amazon S3 encryption client can be used 

------------------------------STORAGE----------------------------------------------------------------------------
Types of Storage 

1) Block Storage
hard disk
typical, one block storage device can be connected to one virtual server
can connect multiple different block storage devices to to one virtual server
Direct Attached Storage: like HD
Storage Area Network SAN
high speed network
connecting a pool of storage devices 
ie used by DB , oracle and SQL server

EBS
Amazon Elastic Block Store 
Instance store
 
2) File Storage
shared with multiple virtual servers
 user need a  quick way to share files 
File Storage MANY to MANY Virtual Server
Amazon Elastic file server EFS (Linux instances)
Amazon Elastic file server EFx (Windows instances)
Amazon FSx for Luster (high performance use cases)


EC2 instance can be attached to 

1) instance stores
physical attached to EC2 instance (on the host computer where EC2 instances are availbe)
used for temporary data (ephemeral storage)
data lost when hardware/instance fail
life-cycle tied  to EC2 instance
used for cache or scratch files
data is not lost on instance reboot
only some ec2 instance types supports instance store

Advantages
v fast I/o
No extra cost 
ideal for storing temporary info

DisAdvantages
slow boot up up to 5 mins
Cannot take a snapshot or restore
Size is fixed based on instance type
cannot attach/detach to other EC2 instance

2) Elastic Block Store
Network Storage
Attached to EC2 instance as a network drive
More durable 
Life cycle not tied to EC2 instance
Provisioned capacity
Highly Flexible increase size when u need
Can be  attach/detach to other EC2 instance
999s availability and replicated within same AZ
UseCase: run your own database

root volume is where OS is stored and it is available by default
--------------
EC2 instance

EBS Instance A in AZ 	ap-south-1a - root volume
EBS Instance B in AZ     ap-south-1a - root volume and secondary volume
EBS Instance C in AZ     ap-south-1b root volume

name: EBS Instance A
region = ap-south-1
Add Storage:only root
Delete on Termination = yes  delete this root volume when EC terminates

name: EBS Instance B
region = ap-south-1
Add Storage: EBS 

name: EBS Instance C
region = ap-south-b
Add Storage: only root

*** Volumes in an availability zone CAN ONLY be attached to EC2 instances in the SAME AZ
A volume to be deleted needs to be first detached
But we can't detached a root volume


lsblk
xdva1 is mounted to /   (root)
cd / (we are on root vol)
xsvdb is not mounted yet

sudo file -s /dev/sdb (from console)

create a file system on xvdb and mount it 
sudo mkfs -t xfs  /dev/xvdb
sud o mkdir /data
sudo mount /dev/xvdb /data

cd /data
touch firstfileonebs.txt


cd  ..
sudo umount /data


Now disconnect a secondary vol from insatnce B and attach it to another ec2 A

Now open Instance A
No need to create the file system as it is already created

sudo file -s /dev/sdf
lsblk
sudo mkdir /data
sudo mount /dev/xvdf /data
cd /data
ls
firstfileonebs.txt

SSD Solid State Drive
Transactional workloads
recommended for boot volumes

HD Hard Disk Drive
Good at  small,random I/O
low cost
Large streaming or big data workloads
Not recommended for boot volumes


EBS SSD Types
*** IO performance increases with size
IOPS increases per GB upto a limit 

1) General Purpose SSD (gp2)
balance price and performance for transactional workloads

small/medium DB dev env

2) Provisioned IOPS SSD (io1)
low latency transactional workloads
large relational or nosql db
high performance at high cost

Lower latency refers to a minimal delay in the processing of computer data over a network connection.
Latency is the amount of time a message takes to traverse a computer network. It is typically measured in milliseconds

EBS HDD Types

1) Throughput Optimised HDD (st1)
Frequently accessed,
Throughput intensive sequential workloads
MapRequce,Kafka, log processing , DWH and ETL

2) Cold HDD (sc1)
lower cost
infrequent data access, eg very low transaction databases

Imp
196. Step 12 - Comparing Four EBS Storage Types

IOPS = input output operations per second
OPS (input/output operations per second) is the standard unit of measurement for the maximum number of reads and writes

I/O is the number of accesses to the dis
IOPS is I/O per second.
Throughput is the amount of info you read in each I/O

if you have a big mouth (big throughput), then you will need less bites and less I/O
Use cases:

Work loads -> usually General Purpose Volume
Databases -> usually IOPS (small data but frequently retrieved)
Big Data / Data warehouses -> usually Throughput ( big data files)
Cold HDD -> Cold File Servers (lowest IOPS before moving to Magnetic)

--------------- EBS SnapShot------------------------------------+
backup of EBS volume
volume can be modify
Volume type and size even it is attached with an EC2 instance

point-in-time snapshots (stored in S3) Aysnc process
Can't accessed directly from S3 but from EC2 API

Volumes- Action -Create snapshot
Can't add encryption here
We can crate a volume from this snapshot as well and can add encryption as well

Snapshots are incremental
You can create multiple snapshots of a volume snapshot1,snapshot2
but snapshot2 only have data after snapshot created and not the previous one
*** We can't loose data if we delete older snapshot
deleting snapshot only delete data which is not needed by other snapshots
snapshot can be shared with other accounts / public

Volumes are constrained to  AZs
Snapshots created from those volumes are constrained to regionals
so to move a volume to another Region, create
snapshot of it

Create a volume 
vol1  in eu-west-1a

Create snapshot from vol1
vol1-snapshot1

Select vol1-snapshot1 - Action - Copy

Fast Snap restore 
eliminates need for pre-warming ie accessing data soon from the volume
Snapshot - Action -  Manage FastSnapshot restore

EEBS Encryption

using AWS KMS
AES-256
Turning on encryption automatically encrypts

Data at rest
	data vols, boot vols and snapshots
Data in Transit
	b/w EC2 and EBS
	b/w EBS and EBS snapshots

Enable encryption at 
When EC2 instance is created you can 
When copy a snapshot 
when copy an AMI

Getting faster I/O Performance b/w EC2 and EBS

1) Launch EC2 as EBS Optimised Instances
during instance creation

2) Enhanced networking through ENA Elastic Network Adapter
increase throughput
need custom configurations

3) using EFA Elastic Fabric Adapter
not available for windows
EFA= ENA + OD Bypass
High Performance Computing

EC2 Life Cycle

Hibernate  max 60 days
data in memory is persisted in EBS volume
only ebs backbend instances can be stopped or hibernated

Increase 

RAID
redundant array of independent disks

RAID 1
when higher \durability is required
data is duplicated

RAID 0
Higher IOPS or storage is required
issue is data  can be lost if one disk fail
use when I/O performance is more important

We can create AMI from a snapshot of a root volume

Scenario
1) use an AMI belonging to a different AWS account or in diff region
a) Owner of AMI provides read permission to AMI
b) for encrypted AMI, owner should share encryption keys
c)copy AMI to the other regions

If u don't have permissions to cipy AMI but have permission to use it 
Sol:  
create EC2 instance from that AMI 
create new AMI from EC2 instance and copy it 

2) Can I attach EBS volume in us-east-1a to EC2 instance in us-east-1b
No, it should be in SAME AZ as of EC2 instance

3) Attach multiple EBS volume to an EC2 instance
Yes

4) Attach an EBS volume to two EC2 instances (at the same time)
No

5) Switch EBS vol from EC2 to another Ec2
Yes, detach and attach

6) Will an EBS volume be immediately available when attached to an EC2 instance ?
Yes, however , by default data is lazily loaded

7) How to endure EBS vol is deleted when EC2 is terminated
Enable delete on termination

8) retain EBS vol even if EC2 backed instance fail
On termination all data on root is lost even if EBS backed
so detach EBS vol before terminating the instance and reover data by connecting 
it to another eC2 instance
use snapshot

9) How to Create EBS volume from EBS volume in diff AZ in same region
create snapshot
create EBS volume from that snapshot and select diff AZ

10) How to Create EBS volume from EBS volume in diff Region 
take a snapshot
copy snapshot to other region
create ebs volume in other region

11) Lowest cost option to maintain snapshots with EBS
store just latest snapshot and delete others

12) how to encrypt and un encrypted EBS vloume
can't do directly
create a snapshot from this volume
create vol from this snapshot with encryption

13) how to automate the complete lifecycle
creation, retention and deletion  of EBS snapshots
use EBS- Amazon Data LifeCycle Manager

--------------------------EFS--------------------------------------------------------
Elastic File System
Can attach a EFS with multiple EC2 (may be in diff Az ) but in a single Region
petabyte scale , auto scaling shared file system

In EBS for auto scaling , you need to provisioned disk and charge for it
In EFS just add files and changed for those files like S3

** Compatible with Amazon EC2 LINUX based instances
use Max I/O Mode for higher throughput (with small latency) 
use case: home dirs, file share, content mgmt, media workflows


Amazon FSX for Lustre
its a File system Optimised for Performance
HPC high performance computing
machine learning, media processing 
Integrates with S3
POSIX complicit
File system data is automatically encrypted at rest and in-transit


Amazon FSX Windows File Servers
fully managed windows file server
SMB service message block protocol
AD integration
File system data is automatically encrypted at rest and in-transit
*All File sharing options are accessible on AWS or on premises


Console - EFS - Create File system
it is associated with a specific VPC

select Security group : EC2 Security Group
to all mount targets

life cycle policy = optional

Throughput Mode
Bursting Mode for most file system
Provisioned

Performance Mode
General Purpose
Max I/O

File System Policy

Access Points
to provide application to access your filesystem

Sharing File System b/w EC2 instances

Edit EC2 Security Group for Inbound
NFS
Custom - EC2 Security Group
thus allow NFS from all EC2 instances with this SG

Mouting your File system

on each EC2 instance
sudo yum install -y amazon-efs-utils
sudo mkdir efs
sudo mount -t efs fs-a440c755:/ efs
cd /efs
sudo touch firstEFsFile.txt
 

Console --> FSX
use us-east-1

Make Optimum Use of Resources in The Cloud
Terminate EC2 instances
Delete all unused EBS volumes, snapshots
Delete all your AMIs
Delete your EFS file storage servers

-----------------------Storage Gateway -------------------------------------------------
Hybrid Storage  (On Premise and on Cloud )
Storage Gateway 
unlimited storage with good performance
Storage Gateway and S3 glacier encrypt data by default

three options
1) Storage File Gateway
2) Storage Tape Gateway
3) Storage Volume Gateway

VM Image with storage gateway software deployed on-premises

1) Storage File Gateway
File gateway deployed as VM on premises
maintains a local cache with most recently used objects
File share (NFS or SMB)  + It benefits from S3 features and integrations

2) AWS Storage Tape Gateway
used for archives
avoid physical tape backup manual process
no need to change tape backup infrastructure
backup data to virtual tapes ie S3 and glacier
S3 life cycle mgmt

3) Storage Volume Gateway
move block storage to cloud
reduce cost
mostly for backup and DR
migration of application data

Cache storage
primary data stored  at AWS S3
on premise cache store frequently accessed data

Volume Stored
High Performance
Primary data store at On premise
async copy to AWS
stored as EBS snapshots
for DR and restore EBS volumes

scenarios: want to use an AMI belonging to a diff AWS account or a diff Region
AMI are restricted to a region


File Storage used for HPC High performance computing  is Amazon FSx for Lustre

--------------------------DATABASES--------------------------------------------
DB provides Organised persistent storage

If DB down 

Availability 
app will not communicate with DB
measured as % of time app provides its operations as expected
4 9's is very good

Durability
data will be lost
measured as will my data be available after 10 years
11 9's is very good

Add DB snapshot after an hour to other data centre
Add transaction log (changes after taking snapshot)
You can setup DB from latest snapshot and apply transaction logs

Use Standby DB in 2nd DC with Sync Replication
Create snapshot from standby DB so that performance will not impact

DownTime 
99.95%
22 mins in a month

99.99% 4 9's
4 and .5  mins in a month

99.999% 5 9's
26 sec in a month

11 9's durability means
store 1 million files for 10 million years, you would expect to lose 1 file

Increase Availability
standbys in Multiple AZ and multiple Regions

Increase Durability 
Multiple copies of data (standby,transaction logs, replicas) in multuple Az and Regions

RTO (Recovery Time Objective)
is the maximum length of time after an outage that your company is willing to wait for the recovery process to finish

RPO Recovery point objective
is the maximum amount of data loss your company is willing to accept as measured in time

very small data loss (RPO 1 min)
very small data loss (RTO 5 min)
Hot Standby (automatically sync data, failover),standby ready

very small data loss (RPO 1 min)
downtime can be tolerate (RTO 15 min)
Warm Standby (automatically sync data,standby with min infra)

Data is critical (RPO 1 min)
downtime (can be few hours)
Regular data snapshots and transaction logs
crate DB from snapshots/transaction logs when a failure happs

Data Can be loss (cached data)
failover to a completely server

Scenario 

Reporting/Analytic is required on DB 
issue: will impact DB performance
Sol:
vertical scale 
DB cluster (expensive)
Create read replicas (async replication)

Actual App will read/write to DB
Now reporting app will read from Read replica
Create read replica in multiple regions

Consistency 
data is updated simultaneously in (standbys and replicas)

Strong Consistency
sync replication to all replicas
will be slow if u have multiple replicas/standbys

Eventual Consistency
Aysnc replication 
eg. social media posts

Read-After-Write Consistency
Insert are immediately available
update/delete are eventually consistent
eg. S3


Choosing type of DB
1) fixed schema or schema less
2) Transactional  properties (automaticity and consistency)
3) latency requirements 
4) TPS
5) how much data to store 

Relational Database
predefined schema
strong transactional capabilities

OLTP
large no. of small transactions
eg. ERP, CRM, Banking
MySQL,Oracle , 
Amazon RDS 
Recommended AWS Service  Aurora (based on PostgreSQL)
each table row is stored together
efficient for processing small transactions

OLAP
analyse petabytes of data
Eg. reporting apps, DWH, business intelligence, 
Data is consolidated from multiple transactional databases
Recommended AWS Service  Redshift based on PostgreSQL
use columnar storage
each table column is stored together
high compression
distribute data , one table in multiple cluster nodes
complex queries can be executed efficiently

Document Databases
data stored as set of documents (whole json)
structure data the way you application needs it
one table instead of dozens
schema-less   semi structure data
useCase: content mgmt, catalogues, user profiles
adv. horizontal scaleable to TB with ms response million of TPS
Recommended AWS Service  DynamoDB

Key-value
use simple key-value pair 
key is unique, value can be obj or simple data values
adv. horizontal scaleable to TB with ms response million of TPS
Recommended AWS Service  DynamoDB
useCase: shopping cart,gaming apps, v. high traffic web apps

Graph DB
store and navigate data with complex relationships
eg fraud detection , fb, social networking data 
Recommended AWS Service  Neptune

In Memory Databases
microsecond latency
storing persistent data in memory
Recommended AWS Service
supports
Redis  for persistent data
Memcached for simple cache

use cases: session management, geospatial apps

DB Scenarios

A start up wih quickly evolving tables  ElastiCache
DynamoDB

Transaction app need to process millions of TPS
DynamoDB

Very high consistency of data required while  processing thousands of TPS
RDS

Cache data from db for a web app
ElastiCache

Relational DB for analytical processing of Petabyte of data
RedShift




















