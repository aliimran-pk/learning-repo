--------------------------DATABASES--------------------------------------------
DB provides Organised persistent storage

If DB down 

Availability 
app will not communicate with DB
measured as % of time app provides its operations as expected
4 9's is very good

Durability
data will be lost
measured as will my data be available after 10 years
11 9's is very good

Add DB snapshot after an hour to other data centre
Add transaction log (changes after taking snapshot)
You can setup DB from latest snapshot and apply transaction logs

Use Standby DB in 2nd DC with Sync Replication
Create snapshot from standby DB so that performance will not impact

DownTime 
99.95%
22 mins in a month

99.99% 4 9's
4 and .5  mins in a month

99.999% 5 9's
26 sec in a month

11 9's durability means
store 1 million files for 10 million years, you would expect to lose 1 file

Increase Availability
standbys in Multiple AZ and multiple Regions

Increase Durability 
Multiple copies of data (standby,transaction logs, replicas) in multuple Az and Regions

RTO (Recovery Time Objective)
is the maximum length of time after an outage that your company is willing to wait for the recovery process to finish

RPO Recovery point objective
is the maximum amount of data loss your company is willing to accept as measured in time

very small data loss (RPO 1 min)
very small data loss (RTO 5 min)
Hot Standby (automatically sync data, failover),standby ready

very small data loss (RPO 1 min)
downtime can be tolerate (RTO 15 min)
Warm Standby (automatically sync data,standby with min infra)

Data is critical (RPO 1 min)
downtime (can be few hours)
Regular data snapshots and transaction logs
crate DB from snapshots/transaction logs when a failure happs

Data Can be loss (cached data)
failover to a completely server

Scenario 

Reporting/Analytic is required on DB 
issue: will impact DB performance
Sol:
vertical scale 
DB cluster (expensive)
Create read replicas (async replication)

Actual App will read/write to DB
Now reporting app will read from Read replica
Create read replica in multiple regions

Consistency 
data is updated simultaneously in (standbys and replicas)

Strong Consistency
sync replication to all replicas
will be slow if u have multiple replicas/standbys

Eventual Consistency
Aysnc replication 
eg. social media posts

Read-After-Write Consistency
Insert are immediately available
update/delete are eventually consistent
eg. S3


Choosing type of DB
1) fixed schema or schema less
2) Transactional  properties (automaticity and consistency)
3) latency requirements 
4) TPS
5) how much data to store 

Relational Database
predefined schema
strong transactional capabilities

OLTP
large no. of small transactions
eg. ERP, CRM, Banking
MySQL,Oracle , 
Amazon RDS 
Recommended AWS Service  Aurora (based on PostgreSQL)
each table row is stored together
efficient for processing small transactions

OLAP
analyse petabytes of data
Eg. reporting apps, DWH, business intelligence, 
Data is consolidated from multiple transactional databases
Recommended AWS Service  Redshift based on PostgreSQL
use columnar storage
each table column is stored together
high compression
distribute data , one table in multiple cluster nodes
complex queries can be executed efficiently

Document Databases
data stored as set of documents (whole json)
structure data the way you application needs it
one table instead of dozens
schema-less   semi structure data
useCase: content mgmt, catalogues, user profiles
adv. horizontal scaleable to TB with ms response million of TPS
Recommended AWS Service  DynamoDB

Key-value
use simple key-value pair 
key is unique, value can be obj or simple data values
adv. horizontal scaleable to TB with ms response million of TPS
Recommended AWS Service  DynamoDB
useCase: shopping cart,gaming apps, v. high traffic web apps

Graph DB
store and navigate data with complex relationships
eg fraud detection , fb, social networking data 
Recommended AWS Service  Neptune

In Memory Databases
microsecond latency
storing persistent data in memory
Recommended AWS Service
supports
Redis  for persistent data
Memcached for simple cache

use cases: session management, geospatial apps

DB Scenarios

A start up wih quickly evolving tables  ElastiCache
DynamoDB

Transaction app need to process millions of TPS
DynamoDB

Very high consistency of data required while  processing thousands of TPS
RDS

Cache data from db for a web app
ElastiCache

Relational DB for analytical processing of Petabyte of data
RedShift

--------- Amazon RDS----------------

RDS is a managed service
For a managed service , you no need to worry about
setup, backup, scaling , replication and upgrade patching 
point in time restore, multi AZ setup for DR
storage backed by EBS
But you can't ssh in the underlying EC2 of RDS

AWS supported Database Engine Types
* Amazon Aurora (postgreSQL + MySQL) not for free tier
PostgresSQL
MYSQL (InnoDB storage engine full support)
MariaDB (enhanced mysql for enterprises)
Oracle DB
Microsoft SQL Server

Multi AZ deployments (standby in another AZ)

Read Replica
Same AZ, Multi AZ, Cross Region
auto scaling storage

Automated backup
daily full backup , transaction log are backup every 5 mins
7 days retention can be increased to 35 days

Manual snapshots

manually triggered by the user
retention of backup for as long as u want


AWS Responsible 
availability (as per your configurations)
durability
scaling  (as per your configurations)
Maintenance patches 
backups 

You cannot 
ssh in db ec2 instance or setup custom software
not allow to install db patches

You can 
manage db users
app optimisation


*  ----------- Multi AZ Deployments ------------------
Mainly used for Disaster Recovery
Standby created in a diff AZ with one DNS name  for failover
Increase Availability
Synchronous replication
No downtime when DB is converted to MultiAz
Not used for scaling
for patches
apply patches at standby and then switch primary with standby
standby is automatically deleted when u delete the DB
* Read Replicas can be setup as Multi AZ for DR

* ----------------- Read Replicas ------------------------------
RR used to scale your read
it supports read-heavy database workloads 
use case: reporting, DWH
can be in same AZ , diff AZ or diff Region
App can connect to them
Create read replicas of a read replica
use Async replication (eventually consistency ie with delay)
Read Replicas and Manual Snapshots are Not deleted when DB is deleted , needs to delete it manually
Must to enable automatic backups before creating read replicas
reduce replication lag by Vertical Scaling
Read replicas are used for Select only stmt
Max No of replicas
* MySQL,mariaDB,PostgresSQL, Oracle = 5
Aws Aurora = 15
SQL Server not supported read replicas

There is Network Cost when data goes from one AZ to another AZ
DB and Read Replica  if  Within same  AZ  = Free


Services -> RSD
Create Database
standard create
Engine Type: MySQL
Template: Free tier
Instance: mysqldbInstance1 (should be unique across Region)
admin / admin123
Brustable Classes  db.t2.micro
SSD 10GB
Default AVP
public access = Yes
Security Group = rds-security-group
Initial database name: todos  (schema)

EC2 instance
Name: TestInstnace
Security Group:  my-EC2-security-group

edit  rds-security-group 
inbound : add  Custom -- my-EC2-security-group

Enable Deletion protection
can't delete DB unless mark this un-checked

End point 
mysqldbinstance1.cellrtm8cnov.ap-south-1.rds.amazonaws.com

DBvisulizer URL
jdbc:mysql://mysqldbinstance1.cellrtm8cnov.ap-south-1.rds.amazonaws.com:3306/todos?characterEncoding=latin1&useConfigs=maxPerformance

Downloao SQL Electron
A simple and lightweight SQL client desktop/terminal
https://sqlectron.github.io/


cd C:\Users\AliImran\Box Sync\P52\My-Learning\My-AWS\Architect\in28Minutes-AWS\LABS
ssh -i "EC2-keypair.pem" ec2-user@ec2-13-233-198-190.ap-south-1.compute.amazonaws.com

sudo yum update

To install mysql client
sudo yum install mysql
msql --version

mysql --host=mysqldbinstance1.cellrtm8cnov.ap-south-1.rds.amazonaws.com --user=admin  --password todos

create table users ( id integer, username varchar(30) );
insert into users values(1, "Ranga");
select * from users;



RDS -Security and Encryption

1) At Rest Encryption
encrypt master and read replica with AWS KMS - AEC-256 encryption
Encryption has to be defined at launch time
* If master is not encrypted, the read replica can't be encrypted
Transparent data encryption TDE is available for Oracle and SQL server

2) In flight Encryption
SSL certificates to encrypt data to RDS  in flight
To enfore SSL
PostgresSQL : rds.force_ssl=1
MySQL : GRANT USAGE ON *.* TO 'mysqluser'@'%' REQUIRE SSL;

create a VPC private subnet
security groups to control access
option to use IAM authentication 
Enable encryption using keys from KMS
when Encryption is enabled data in database ,automated backups, read replicas and snapshots are all Encrypted
Use SSL certificates to encrypt data in flight (EC2 to RDS)

snapshots (backups) of un-encrypted RDS databases are un-encrypted
snapshots (backups) of encrypted RDS databases are encrypted

To Encrypt an un-encrypted RDS database
create a snapshot of  un-encrypted  db
copy the snapshot and enable encryption for the snapshot
restore the database from encrypted snapshot
migrate application to new db and delete old db

RDS databases are usually deployed within a private subnet
leveraging DG
IAM policies help control who can manage AWS RDS through RDSAPI ie 
who can create db, delete db

IAM Authentication
works with MySQL and PostgreSQL only
no need password, just a token obtained through IAM and RDS API calls
valid for 15 mins
IAM to centrally manage users instead of DB

RDS Costs

1) DB instance hours
2) Storage per GB per month  , you provisioned and not usage
Backups and snapshot storage
data transfer cost
Not with in AZ
With outside Region

-----------------------------------Amazon Aurora ----------------
It is propriety of AWS and not open sourced
AWS cloud optimized and claim 5X performance over MySQL and 3X over Postgres
Storage automatically grow inclemently of 10GB up to 64GB
15 replicas
Instance failover
Cost is 20% than RDS but is efficient

it maintains 6 copies of  your data across 3 AZ
4 copies out of 6 for writes
3 copies out of 6 for reads
replication + self healing + auto expanding  
Uses Cluster shared storage volumes (multi AZ storage)
it creates cluster of volumes spread across multiple AZ 
Primary instance  read/write to cluster vol and Replicas read from cluster volume
provides Global database options (multiple regions)
support for cross region replication
Backtrack: restore data at any point of time without using backups
Aurora Security similar to RDS as it uses same engine

Deployment Option

Client use writer endpoint pointing to master which writers
Client use reader endpoint pointing to readers cluster (load balancing) for read

1) Single Master 
one writer and multiple readers

2)  Multi Master 
multiple writers

3) Aurora Serverless
Automated DB instantiation and auto scaling
No need to provide the size and capacity planing
min or max to be provide and aws scale automatically
* good for infrequent, irregular and unpredictable workloads
pay per second and can be more cost effective

Client connect to proxy fleet (managed by Aurora)

GLOBAL  AURORA
1) Cross Region Read replicas
use for DR
simple to put in place

2) Aurora Global Database (recommended)

One primary region (read + write)
up to 5 secondary (read-only ) regions, replication lag < 1 second
up to 6 read replicas per secondary region
rto < 1 min for DR in other region

RDS -->
Standard Create
Amazon Aurora
with MYSQL compatibility
Version: 5.6.10a

Database location
Regional or Global

Templates
Dev/Test

--------------------------------------------------------------------------------------------------

Maria DB
MySQL Compatible DB

Oracle

select Edition
some has license included, some not 

SQL Server
License is included 

RDS Scaling
* Normally manual scale up to 64TB 
SQL Server up to 16TB
storage and compute typically applied during maintenance window or you can choose app-immediately

Horizontal Scaling
configure read replicas
For Aurora (multi-master, writer with multiple  readers)

Use cloud watch for historical data
configure cloudwatch alarms when near max capacity
slow queries  , enable enhanced monitoring
autonomic backup during backup windows in S3 , default retain 7 days, max 35 days
Achieve RPO up to 5 mins


When to use RDS
1) pre-defined schema
2) where strong transactional capabilities and complex queries required

RDS is Not  recommend for 
Highly scalable massive read/write eg. millions of writes/sec  (go for DynamoDB)
Upload files using Get/PUT Rest API (use S3)
heavy customisation for DB or need to access underlying EC2 (Go for custom DB installation)

Migrate on-premise database to cloud database of same type
AWS Database Migration Service

Migrate data from one DB engine to other
use AWS Schema Conversion tool

reduce global latency and improve DR
use multi region read replica

select subnets a RDS instance is launched into
create DB subnet groups

Add encryption to an unencrypted db instance
create DB snapshot
encrypt the snapshot using keys in KMS
create database from encrypted snapshot

Billed if DB is stopped 
Only for storage, IOPS , backups and snapshots
Not billed for DB instance hours

Need RDS for an year, reduce cost 
use RDS reserved instances

Efficiently manage DB connections
use AwS RDS Proxy
sits b/w client app (including lambda) and RDS

------------------AWS ElasticCache-------------------------------------------
Managed Service
Highly scalable and low latency in-memory data store
u can store in memory data in EC
as a distributed caching solution

WRITE scaling using sharding
READ scaling using Read Replicas
Multi AZ with failover 

 Cash Miss
 get data from db

 Cash Hit
 found data in the elastic cache
 
 User Session Store
 user logs into a application 
 application writes session data into ElasticCache
 if user hits another appl instance then its session is retrieved from EC
 
 
 All caches in ElasticCahce supports SSL in flight encryption
 *** Don't support IAM authentication
 Redis AUTH
 ow can you enhance the security of your Redis cache to force users to enter a password?
 can set password/token when u create a redis cluster
 
 memcached supports SASL based authentication
 
 
 Redis and Memcached are two Custer engine for ElasticCache
 
1) Redis
in memory persistence data store
* Multi AZ deployments with automatic failover
* durability using AOF persistence
*  supports backup (in S3) and restore
can schedule snapshots
configure backup windows
encryption at rest  (KMS) and in transit 
use cases: caching,session store,chat  messaging, geopolitical apps, queues
shard = collection of one or more nodes (where a portion of ur data is available)
One node act as read/write Primary
Other nodes act as read replicas (up to 5 read replicas)
In case of Failure:
Primary node is replaced
if Multi-AZ replication group is enabled, read replica is promoted to primary
publish subscribe messing  (act as a  message broker)
read replicas and failover support
encryption support
can be used as database

2) Memcached
pure caching sol
distributed 
Multi Node for partitioning of data (sharding)
** Non persistent cache
multi threaded architecture
simple key value storage
ideal for front end for data stores like RDS and DynamoDB
can be used as a Transient  session store
create up to 20 cache nodes
use Auto discovery to discover cache
Low maintenance simple caching solution
Easy auto scaling
Limitations
No backup or  restore supported
No encryption or replication or snapshot
when node fails, all data in that node is lost 
reduce impact of failure by using Large no. of small small nodes

Cluster Engine : Memcached


Patterns for ElasticCache

1) Lazy Loading
all read data is cached, data can become stale in cache

2) Write Through
add or update data in cache when written to DB (No stale data)

3) Session Store
store temporary session data in cache using TTL feature
Storing Session Data in ElastiCache is a common pattern to ensuring different instances can retrieve your user's state if needed. 

Multi AZ ensure High Availability
Global Databases allow you to have cross region replication

------------------------ AWS DynamoDB --------------------------------------------------------
fast, scalable distributed db for any scale
schema less
No sql key value and document based
Don't worry about scaling , availability or durability
it automatically partitioned data as it grows across multiple nodes
single-digit millisecond latency at any scale.
3 replica in a single region
No need to crate a DB 
here create a table directly and configure RCU and WCU Read capacity unit
provides a expensive serverless  mode
application which required milli second latency but at v high scale

Hierarchy 
Table ->Items -> attributes (key value pairs)
Mandatory Primary Key
Max 400KB item size

DyamoDB 
table name: todo
Primary key : id  (mandatory) used to distribute the data across different partitions
Add sort key 

TTL time to live 
expiry of record

Manage Stream
ie upon any event execute stream,

scan is expensive operation which query is efficient
based on specific metric , you can create alarms

if you need to create the query by a column other than Id then create an index of that column

keys in Json is case sensitive 
eg. Address and address will create two columns#

{
"Address": "Lahore",
"id": "001",
"name": "Ali"
}


Create Index
Key : Address
Name: Address-index

Global tables
enable you to use DynamoDB as fully managed , multi region, multi-master database
to create global table , must enabled dynamoDB streams

Triggers 
connect DynamoDB streams to Lambda functions whenever an item in the tale is modified , a new
stream record is written, which in turn invoke the lambda function

Access control
helpful for direct database access by mobile apps, web identify federation allows your
mobile apps to use identity providers as Login with Facebook, Google

partition key is mandatory for search
can't search using only sort key

partition key + sort key = composite pk

DynamoDB Indexes

Local Secondary Index
Same partition key  as of Primary Key but different sort key
Should be created at the table creation

Global Secondary Index
Partition and sort keys are diff from Primary Key
Can be added or removed at any point in time
stored separately from original table


Query VS Scan

Query
search using partition key (PK or Index) and a distinct value to search
optional: sort key and filters
results are sorted by PK
Max 1 MB result returned

SCAN
read every item in a table
expensive 
return all attributes by default
support paging above 1MB
Filter items using expressions

Consistency Levels
Eventually consistent (1 sec lag by default)
Request for strongly consistent reads
Set ConsistentRead to true (slow and expensive)
supports Transactions but cost will be twice

Read/Write Capacity Modes

In DynamoDB, strongly consistent reads are expensive than eventually consistent reads


Provisioned
Recommended
provision read and write capacity
dynamically adjustable
unused capacity can be used in burst(in case of spike)
Billed for provisioned capacity  irrespective of whether you used it or not

On Demand
Truly serverless and expensive
For unknown workloads or traffic with Huge spikes
used when workloads are really spiky causing low utilization of provisioned capacity
or usage is very low 

Dynamo DB RCU and WRC
Capacity used depends on size of item, read consistency , transnational etc
1 capacity unit to read 4KB or smaller 
1 capacity unit to write 1KB or smaller 
twice the capacity for strong consistent or transactional request
On Demand RCU is 8 times the cost of Provisioned RCU

Performance Monitoring
use cloudwatch
alerts on RCU,WCU and throttle request

Migration from RDS or MongoDB to DynamoDB
AWS Migration Service
Enable point-in-time recovery (35 days)
use Time to Live TTL  to automatically expire items

IAM and Encryption
Server side encryption with KMS keys is Always enabled (automatically encrypt tables, streams and backups)

Client Side encryption
DynamoDB Encryption Client

Use IAM roles to provide EC2 instances or AWS services access to DynamoDB tables
predefined polices 
AmazonDynamoDBReadOnlyAccess
AmazonDynamoDBFullccess
Fine-grained control at the individual item level


DynaymoDB
milli sec latency with millions TPS but lower consistency
Rest API, SDK, CLI
Difficult to run complex queries
No upper limit

RDS
stronger consistency and transactional capabilities
SQL Queries
Good to run complex queries
upper Limit 64TB


DynamoDB Accelerator (DAX)
In memory cache for DynamoDB
microsecond response time

Few changes needed to connect to DAX
can reduce your costs by saving our read capacity units (lambda reads from DAX rather than hititng DynamoDB)

Not Recommanded
if u need strongly consistent reads
application is write intensive with very few reads

DynamoDB -> DAX 
create cluster
encryption is recommended

