--------------------------DATABASES--------------------------------------------
DB provides Organised persistent storage

If DB down 

Availability 
app will not communicate with DB
measured as % of time app provides its operations as expected
4 9's is very good

Durability
data will be lost
measured as will my data be available after 10 years
11 9's is very good

Add DB snapshot after an hour to other data centre
Add transaction log (changes after taking snapshot)
You can setup DB from latest snapshot and apply transaction logs

Use Standby DB in 2nd DC with Sync Replication
Create snapshot from standby DB so that performance will not impact

DownTime 
99.95%
22 mins in a month

99.99% 4 9's
4 and .5  mins in a month

99.999% 5 9's
26 sec in a month

11 9's durability means
store 1 million files for 10 million years, you would expect to lose 1 file

Increase Availability
standbys in Multiple AZ and multiple Regions

Increase Durability 
Multiple copies of data (standby,transaction logs, replicas) in multuple Az and Regions

RTO (Recovery Time Objective)
is the maximum length of time after an outage that your company is willing to wait for the recovery process to finish

RPO Recovery point objective
is the maximum amount of data loss your company is willing to accept as measured in time

very small data loss (RPO 1 min)
very small data loss (RTO 5 min)
Hot Standby (automatically sync data, failover),standby ready

very small data loss (RPO 1 min)
downtime can be tolerate (RTO 15 min)
Warm Standby (automatically sync data,standby with min infra)

Data is critical (RPO 1 min)
downtime (can be few hours)
Regular data snapshots and transaction logs
crate DB from snapshots/transaction logs when a failure happs

Data Can be loss (cached data)
failover to a completely server

Scenario 

Reporting/Analytic is required on DB 
issue: will impact DB performance
Sol:
vertical scale 
DB cluster (expensive)
Create read replicas (async replication)

Actual App will read/write to DB
Now reporting app will read from Read replica
Create read replica in multiple regions

Consistency 
data is updated simultaneously in (standbys and replicas)

Strong Consistency
sync replication to all replicas
will be slow if u have multiple replicas/standbys

Eventual Consistency
Aysnc replication 
eg. social media posts

Read-After-Write Consistency
Insert are immediately available
update/delete are eventually consistent
eg. S3


Choosing type of DB
1) fixed schema or schema less
2) Transactional  properties (automaticity and consistency)
3) latency requirements 
4) TPS
5) how much data to store 

Relational Database
predefined schema
strong transactional capabilities

OLTP
large no. of small transactions
eg. ERP, CRM, Banking
MySQL,Oracle , 
Amazon RDS 
Recommended AWS Service  Aurora (based on PostgreSQL)
each table row is stored together
efficient for processing small transactions

OLAP
analyse petabytes of data
Eg. reporting apps, DWH, business intelligence, 
Data is consolidated from multiple transactional databases
Recommended AWS Service  Redshift based on PostgreSQL
use columnar storage
each table column is stored together
high compression
distribute data , one table in multiple cluster nodes
complex queries can be executed efficiently

Document Databases
data stored as set of documents (whole json)
structure data the way you application needs it
one table instead of dozens
schema-less   semi structure data
useCase: content mgmt, catalogues, user profiles
adv. horizontal scaleable to TB with ms response million of TPS
Recommended AWS Service  DynamoDB

Key-value
use simple key-value pair 
key is unique, value can be obj or simple data values
adv. horizontal scaleable to TB with ms response million of TPS
Recommended AWS Service  DynamoDB
useCase: shopping cart,gaming apps, v. high traffic web apps

Graph DB
store and navigate data with complex relationships
eg fraud detection , fb, social networking data 
Recommended AWS Service  Neptune

In Memory Databases
microsecond latency
storing persistent data in memory
Recommended AWS Service
supports
Redis  for persistent data
Memcached for simple cache

use cases: session management, geospatial apps

DB Scenarios

A start up wih quickly evolving tables  ElastiCache
DynamoDB

Transaction app need to process millions of TPS
DynamoDB

Very high consistency of data required while  processing thousands of TPS
RDS

Cache data from db for a web app
ElastiCache

Relational DB for analytical processing of Petabyte of data
RedShift

--------- Amazon RDS----------------

For a managed service , you no need to worry about
setup, backup, scaling , replication and upgrade patching 

it supports
Amazon Aurora (postgreSQL + MySQL)
PostgresSQL
MYSQL (InnoDB storage engine full suppot)
MariaDB (enhanced mysql for enterprises)
Oracle DB
Microsoft SQL Server

Multi AZ deployments (standby in another AZ)

Read Replica
Same AZ, Multi AZ, Cross Region
auto scaling storage
automated bacup
manual snapshots

AWS Responsible 
availability (as per your configurations)
durability
scaling  (as per your configurations)
Maintenance patches 
backups 

You cannot 
ssh in db ec2 instance or setup custom software
not allow to install db patches

You can 
manage db users
app optimisation

Services -> RSD
Create Database
standard create
Engine Type: MySQL
Template: Free tier
Instance: mysqldbInstance1
admin / admin123
Brustable Classes  db.t2.micro
SSD 10GB
Default AVP
public access = Yes
Security Group = rds-security-group
Initial database name: todos  (schema)

EC2 instance
Name: TestInstnace
Security Group:  my-EC2-security-group

edit  rds-security-group 
inbound : add  Custom -- my-EC2-security-group


End point 
mysqldbinstance1.cellrtm8cnov.ap-south-1.rds.amazonaws.com

DBvisulizer URL
jdbc:mysql://mysqldbinstance1.cellrtm8cnov.ap-south-1.rds.amazonaws.com:3306/todos?characterEncoding=latin1&useConfigs=maxPerformance


cd C:\Users\AliImran\Box Sync\P52\My-Learning\My-AWS\Architect\in28Minutes-AWS\LABS
ssh -i "EC2-keypair.pem" ec2-user@ec2-13-233-198-190.ap-south-1.compute.amazonaws.com

sudo yum update

To install mysql client
sudo yum install mysql
msql --version

mysql --host=mysqldbinstance1.cellrtm8cnov.ap-south-1.rds.amazonaws.com --user=admin  --password todos

create table users ( id integer, username varchar(30) );
insert into users values(1, "Ranga");
select * from users;

Multi AZ Deployments
standby created in a diff AZ
synchronous replication
No downtime when DB is converted to MultiAz
for patches
apply patches at standby and then switch primary with standby
standby is automatically deleted when u delete the DB

Read Replicas
it supports read-heavy database workloads 
use case: reporting, DWH
can be in same , diff AZ or diff Region
App can connect to them
Create read replicas of a read replica
use Async replication (eventually consistency ie with delay)
Read Replicas and Manual Snapshots are Not deleted when DB is deleted , needs to delete it manually
Must to enable automatic backups before creating read replicas
reduce replication lag by Vertical Scaling
Max No of replicas
MySQL,mariaDB,PostgresSQL, Oracle = 5
Aws Aurora = 15
SQL Server not supported read replicas

-----------------------------------Amazon Aurora ----------------

it maintains 2 copies of data each in min 3 AZ
Uses Cluster volumes (multi AZ storage)
it creates cluster of volumes spread across multiple AZ 
Primary instance  read/write to cluster vol and Replicas read from cluster volume
provides Global database options (multiple regions)
Deployment Option

1) Single Master 
one writer and multiple readers

2)  Multi Master 
multiple writers

3) Serverless
No need to provide the size
min or max to be provide and aws scale automatically


Maria DB
MySQL Compatible DB

Oracle

select Edition
some has license included, some not 

SQL Server
License is included 

RDS Scaling
Normally manual scale up to 64TB 
SQL Server up to 16TB
storage and compute typically applied during maintenance window or you can choose app-immediately

Horizontal Scaling
configure read replicas
For Aurora (multi-master, writer with multiple  readers)

Use cloud watch for historical data
configure cloudwatch alarms when near max capacity
slow queries  , enable enhanced monitoring
autonomic backup during backup windows in S3 , default retain 7 days, max 35 days
Achieve RPO up to 5 mins

RDS -Security and Encryption

create a VPC private subnet
security groups to control access
option to use IAM authentication 
Enable encryption using keys from KMS
when Encryption is enabled data in database ,automated backups, read replicas and snapshots are all Encrypted
Use SSL certificates to encrypt data in flight (EC2 to RDS)

RDS Costs

1) DB instance hours
2) Storage per GB per month  , you provisioned and not usage
Backups and snapshot storage
data transfer cost
Not with in AZ
With outside Region

When to use RDS
1) pre-defined schema
2) where strong transactional capabilities and complex queries required

RDS is Not  recommend for 
Highly scalable massive read/write eg. millions of writes/sec  (go for DynamoDB)
Upload files using Get/PUT Rest API (use S3)
heavy customisation for DB or need to access underlying EC2 (Go for custom DB installation)

Migrate on-premise database to cloud database of same type
AWS Database Migration Service

Migrate data from one DB engine to other
use AWS Schema Conversion tool

reduce global latency and improve DR
use multi region read replica

select subnets a RDS instance is launched into
create DB subnet groups

Add encryption to an unencrypted db instance
create DB snapshot
encrypt the snapshot using keys in KMS
create database from encrypted snapshot

Billed if DB is stopped 
Only for storage, IOPS , backups and snapshots
Not billed for DB instance hours

Need RDS for an year, reduce cost 
use RDS reserved instances

Efficiently manage DB connections
use AwS RDS Proxy
sits b/w client app (including lambda) and RDS


------------------------ AWS DynamoDB --------------------------------------------------------
fast, scalable distributed db for any scale
schema less
No sql key value and document based
Don't worry about scaling , availability or durability
it automatically partitioned data as it grows across multiple nodes
single-digit millisecond latency at any scale.
3 replica in a single region
No need to crate a DB 
here create a table directly and configure RCU and WCU Read capacity unit
provides a expensive serverless  mode
application which required milli second latency but at v high scale

Hierarchy 
Table ->Items -> attributes (key value pairs)
Mandatory Primary Key
Max 400KB item size

DyamoDB 
table name: todo
Primary key : id  (mandatory) used to distribute the data across different partitions
Add sort key 

TTL time to live 
expiry of record

Manage Stream
ie upon any event execute stream,

scan is expensive operation which query is efficient
based on specific metric , you can create alarms

if you need to create the query by a column other than Id then create an index of that column

keys in Json is case sensitive 
eg. Address and address will create two columns#

{
"Address": "Lahore",
"id": "001",
"name": "Ali"
}


Create Index
Key : Address
Name: Address-index

Global tables
enable you to use DynamoDB as fully managed , multi region, multi-master database
to create global table , must enabled dynamoDB streams

Triggers 
connect DynamoDB streams to Lambda functions whenever an item in the tale is modified , a new
stream record is written, which in turn invoke the lambda function

Access control
helpful for direct database access by mobile apps, web identify federation allows your
mobile apps to use identity providers as Login with Facebook, Google

partition key is mandatory for search
can't search using only sort key

partition key + sort key = composite pk

DynamoDB Indexes

Local Secondary Index
Same partition key  as of Primary Key but different sort key
Should be created at the table creation

Global Secondary Index
Partition and sort keys are diff from Primary Key
Can be added or removed at any point in time
stored separately from original table


Query VS Scan

Query
search using partition key (PK or Index) and a distinct value to search
optional: sort key and filters
results are sorted by PK
Max 1 MB result returned

SCAN
read every item in a table
expensive 
return all attributes by default
support paging above 1MB
Filter items using expressions

Consistency Levels
Eventually consistent (1 sec lag by default)
Request for strongly consistent reads
Set ConsistentRead to true (slow and expensive)
supports Transactions but cost will be twice

Read/Write Capacity Modes

In DynamoDB, strongly consistent reads are expensive than eventually consistent reads


Provisioned
Recommended
provision read and write capacity
dynamically adjustable
unused capacity can be used in burst(in case of spike)
Billed for provisioned capacity  irrespective of whether you used it or not

On Demand
Truly serverless and expensive
For unknown workloads or traffic with Huge spikes
used when workloads are really spiky causing low utilization of provisioned capacity
or usage is very low 

Dynamo DB RCU and WRC
Capacity used depends on size of item, read consistency , transnational etc
1 capacity unit to read 4KB or smaller 
1 capacity unit to write 1KB or smaller 
twice the capacity for strong consistent or transactional request
On Demand RCU is 8 times the cost of Provisioned RCU

Performance Monitoring
use cloudwatch
alerts on RCU,WCU and throttle request

Migration from RDS or MongoDB to DynamoDB
AWS Migration Service
Enable point-in-time recovery (35 days)
use Time to Live TTL  to automatically expire items

IAM and Encryption
Server side encryption with KMS keys is Always enabled (automatically encrypt tables, streams and backups)

Client Side encryption
DynamoDB Encryption Client

Use IAM roles to provide EC2 instances or AWS services access to DynamoDB tables
predefined polices 
AmazonDynamoDBReadOnlyAccess
AmazonDynamoDBFullccess
Fine-grained control at the individual item level


DynaymoDB
milli sec latency with millions TPS but lower consistency
Rest API, SDK, CLI
Difficult to run complex queries
No upper limit

RDS
stronger consistency and transactional capabilities
SQL Queries
Good to run complex queries
upper Limit 64TB


DynamoDB Accelerator (DAX)
In memory cache for DynamoDB
microsecond response time

Few changes needed to connect to DAX
can reduce your costs by saving our read capacity units (lambda reads from DAX rather than hititng DynamoDB)

Not Recommanded
if u need strongly consistent reads
application is write intensive with very few reads

DynamoDB -> DAX 
create cluster
encryption is recommended

------------------AWS ElasticCache-------------------------------------------
Managed Service
Highly scalable and low latency in-memory data store
u can store in memory data in EC
as a distibuted caching solution

1) Redis
in memory persistence data store
autoamtic failover with multi AZ deployments
suports backup (in S3) and restore
can schedule snapshots
configure backup windows
encryption at rest  (KMS) and in transit 
use cases: caching,session store,chat  messaging, geopolitical apps, queues
shard = collection of one or more nodes (where a portion of ur data is available)
One node act as read/write Primary
Other nodes act as read replicas (up to 5 read replicas)
In case of Failure:
Primary node is replaced
if Multi-AZ replication group is enabled, read replica is promoted to primary
publish subscribe messing  (act as a  message broker)
read replicas and failover support
encryption support


2) Memcached
pure caching sol
distributed 
non persistent 
simple key value storage
ideal for front end for data stores like RDS and DynamoDB
can be used as a Transient  session store
create up to 20 cache nodes
use Auto discovery to discover cache
Low maintenance simple caching solution
Easy auto scaling
Limitations
No backup or  restore supported
No encryption or replication or snapshot
when node fails, all data in that node is lost 
reduce impact of failure by using Large no. of small small nodes

Cluster Engine : Memcached

